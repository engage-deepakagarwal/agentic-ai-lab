{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen import ConversableAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\"AOAI_CONFIG_LIST\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-4o\"\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"Admin\",\n",
    "    code_execution_config={\n",
    "        \"use_docker\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    llm_config= { \"config_list\": config_list },\n",
    "    system_message=\"\"\"\n",
    "    You are the Engineer. Given a topic, write code to retrieve related papers from the arXiv API, \n",
    "    print their title, authors, abstract, and link.\n",
    "    You write python/shell code to solve tasks. Wrap the code in a code block that specifies \n",
    "    the script type. The user can't modify your code. So do not suggest incomplete code which \n",
    "    requires others to modify. Don't use a code block if it's not intended to be executed by \n",
    "    the executor. Don't include multiple code blocks in one response. \n",
    "    Do not ask others to copy and paste the result. Check the execution result returned by \n",
    "    the executor. If the result indicates there is an error, fix the error and output the code again. \n",
    "    Suggest the full code instead of partial code or code changes. \n",
    "    If the error can't be fixed or if the task is not solved even after the code is executed successfully, \n",
    "    analyze the problem, revisit your assumption, collect additional info you need, and think of a \n",
    "    different approach to try.\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=\"Executor. Execute the code written by the Coder and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "scientist = autogen.AssistantAgent(\n",
    "    name=\"Scientist\",\n",
    "    llm_config={ \"config_list\": config_list },\n",
    "    system_message=\"\"\"\n",
    "    You are the Scientist. Please categorize papers after seeing their abstracts printed \n",
    "    and create a markdown table with Domain, Title, Authors, Summary and Link\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function to select the speaker deterministically as per need "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from autogen import Agent, GroupChat\n",
    "\n",
    "def state_transition(last_speaker: Agent, groupchat: GroupChat):\n",
    "    messages = groupchat.messages\n",
    "    print(f\"Message: {json.dumps(messages)}\")\n",
    "    \n",
    "    if last_speaker is user_proxy:\n",
    "        # init -> retrieve\n",
    "        return engineer\n",
    "    elif last_speaker is engineer:\n",
    "        # retrieve: action 1 -> action 2\n",
    "        return executor\n",
    "    elif last_speaker is executor:\n",
    "        if \"exitcode: 1\" in messages[-1][\"content\"]:\n",
    "            # retrieve --(execution failed)--> retrieve\n",
    "            return engineer\n",
    "        else:\n",
    "            # retrieve --(execution success)--> research\n",
    "            return scientist\n",
    "    elif last_speaker == \"Scientist\":\n",
    "        # research -> end\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, engineer, executor, scientist],\n",
    "    messages=[],\n",
    "    max_round=20,\n",
    "    speaker_selection_method=state_transition,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat, \n",
    "    llm_config={ \"config_list\": config_list }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "Topic: LLM applications papers from last week. Requirement: 5 - 10 papers from different domains.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Message: [{\"content\": \"Topic: LLM applications papers from last week. Requirement: 5 - 10 papers from different domains.\", \"role\": \"user\", \"name\": \"Admin\"}]\n",
      "\u001b[32m\n",
      "Next speaker: Engineer\n",
      "\u001b[0m\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "Sure! I will write a Python script to retrieve papers from the arXiv API based on the topic \"Large Language Models (LLM) applications\" and filter them to get the latest papers from the past week. The script will display the title, authors, abstract, and link for each paper.\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import datetime\n",
      "from dateutil.parser import parse\n",
      "\n",
      "def get_arxiv_papers(query, max_results=10):\n",
      "    base_url = 'http://export.arxiv.org/api/query?'\n",
      "    query = f'search_query=all:{query}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending'\n",
      "    \n",
      "    response = requests.get(base_url + query)\n",
      "    \n",
      "    if response.status_code != 200:\n",
      "        print(\"Error fetching data from arXiv API\")\n",
      "        return\n",
      "    \n",
      "    feed = response.text\n",
      "    start_pos = 0\n",
      "    papers = []\n",
      "\n",
      "    while True:\n",
      "        start_index = feed.find('<entry>', start_pos)\n",
      "        if start_index == -1:\n",
      "            break\n",
      "        end_index = feed.find('</entry>', start_index)\n",
      "        entry = feed[start_index:end_index]\n",
      "        \n",
      "        title_start = entry.find('<title>') + len('<title>')\n",
      "        title_end = entry.find('</title>', title_start)\n",
      "        title = entry[title_start:title_end].strip()\n",
      "        \n",
      "        authors_start = entry.find('<author>')\n",
      "        authors_end = entry.find('</author>', authors_start)\n",
      "        authors = []\n",
      "        while authors_start != -1 and authors_start < authors_end:\n",
      "            author_start = entry.find('<name>', authors_start) + len('<name>')\n",
      "            author_end = entry.find('</name>', author_start)\n",
      "            authors.append(entry[author_start:author_end].strip())\n",
      "            authors_start = entry.find('<author>', author_end)\n",
      "        \n",
      "        abstract_start = entry.find('<summary>') + len('<summary>')\n",
      "        abstract_end = entry.find('</summary>', abstract_start)\n",
      "        abstract = entry[abstract_start:abstract_end].strip()\n",
      "\n",
      "        link_start = entry.find('<id>') + len('<id>')\n",
      "        link_end = entry.find('</id>', link_start)\n",
      "        link = entry[link_start:link_end].strip()\n",
      "\n",
      "        updated_start = entry.find('<updated>') + len('<updated>')\n",
      "        updated_end = entry.find('</updated>', updated_start)\n",
      "        updated = entry[updated_start:updated_end].strip()\n",
      "\n",
      "        papers.append({\n",
      "            'title': title,\n",
      "            'authors': authors,\n",
      "            'abstract': abstract,\n",
      "            'link': link,\n",
      "            'updated': parse(updated)\n",
      "        })\n",
      "\n",
      "        start_pos = end_index\n",
      "    \n",
      "    return papers\n",
      "\n",
      "def filter_recent_papers(papers, days=7):\n",
      "    recent_papers = []\n",
      "    current_time = datetime.datetime.now(datetime.timezone.utc)\n",
      "    for paper in papers:\n",
      "        if (current_time - paper['updated']).days <= days:\n",
      "            recent_papers.append(paper)\n",
      "    return recent_papers\n",
      "\n",
      "query = \"Large Language Models applications\"\n",
      "papers = get_arxiv_papers(query, max_results=20)\n",
      "recent_papers = filter_recent_papers(papers)\n",
      "\n",
      "for idx, paper in enumerate(recent_papers[:10]):\n",
      "    print(f\"Paper {idx + 1}:\")\n",
      "    print(f\"Title: {paper['title']}\")\n",
      "    print(f\"Authors: {', '.join(paper['authors'])}\")\n",
      "    print(f\"Abstract: {paper['abstract']}\")\n",
      "    print(f\"Link: {paper['link']}\")\n",
      "    print()\n",
      "\n",
      "if not recent_papers:\n",
      "    print(\"No recent papers found on this topic.\")\n",
      "```\n",
      "\n",
      "This script fetches up to 20 papers with the query \"Large Language Models applications\" from the arXiv API, filters them to get the ones from the past week, and prints their title, authors, abstract, and link.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Message: [{\"content\": \"Topic: LLM applications papers from last week. Requirement: 5 - 10 papers from different domains.\", \"role\": \"user\", \"name\": \"Admin\"}, {\"content\": \"Sure! I will write a Python script to retrieve papers from the arXiv API based on the topic \\\"Large Language Models (LLM) applications\\\" and filter them to get the latest papers from the past week. The script will display the title, authors, abstract, and link for each paper.\\n\\n```python\\nimport requests\\nimport datetime\\nfrom dateutil.parser import parse\\n\\ndef get_arxiv_papers(query, max_results=10):\\n    base_url = 'http://export.arxiv.org/api/query?'\\n    query = f'search_query=all:{query}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending'\\n    \\n    response = requests.get(base_url + query)\\n    \\n    if response.status_code != 200:\\n        print(\\\"Error fetching data from arXiv API\\\")\\n        return\\n    \\n    feed = response.text\\n    start_pos = 0\\n    papers = []\\n\\n    while True:\\n        start_index = feed.find('<entry>', start_pos)\\n        if start_index == -1:\\n            break\\n        end_index = feed.find('</entry>', start_index)\\n        entry = feed[start_index:end_index]\\n        \\n        title_start = entry.find('<title>') + len('<title>')\\n        title_end = entry.find('</title>', title_start)\\n        title = entry[title_start:title_end].strip()\\n        \\n        authors_start = entry.find('<author>')\\n        authors_end = entry.find('</author>', authors_start)\\n        authors = []\\n        while authors_start != -1 and authors_start < authors_end:\\n            author_start = entry.find('<name>', authors_start) + len('<name>')\\n            author_end = entry.find('</name>', author_start)\\n            authors.append(entry[author_start:author_end].strip())\\n            authors_start = entry.find('<author>', author_end)\\n        \\n        abstract_start = entry.find('<summary>') + len('<summary>')\\n        abstract_end = entry.find('</summary>', abstract_start)\\n        abstract = entry[abstract_start:abstract_end].strip()\\n\\n        link_start = entry.find('<id>') + len('<id>')\\n        link_end = entry.find('</id>', link_start)\\n        link = entry[link_start:link_end].strip()\\n\\n        updated_start = entry.find('<updated>') + len('<updated>')\\n        updated_end = entry.find('</updated>', updated_start)\\n        updated = entry[updated_start:updated_end].strip()\\n\\n        papers.append({\\n            'title': title,\\n            'authors': authors,\\n            'abstract': abstract,\\n            'link': link,\\n            'updated': parse(updated)\\n        })\\n\\n        start_pos = end_index\\n    \\n    return papers\\n\\ndef filter_recent_papers(papers, days=7):\\n    recent_papers = []\\n    current_time = datetime.datetime.now(datetime.timezone.utc)\\n    for paper in papers:\\n        if (current_time - paper['updated']).days <= days:\\n            recent_papers.append(paper)\\n    return recent_papers\\n\\nquery = \\\"Large Language Models applications\\\"\\npapers = get_arxiv_papers(query, max_results=20)\\nrecent_papers = filter_recent_papers(papers)\\n\\nfor idx, paper in enumerate(recent_papers[:10]):\\n    print(f\\\"Paper {idx + 1}:\\\")\\n    print(f\\\"Title: {paper['title']}\\\")\\n    print(f\\\"Authors: {', '.join(paper['authors'])}\\\")\\n    print(f\\\"Abstract: {paper['abstract']}\\\")\\n    print(f\\\"Link: {paper['link']}\\\")\\n    print()\\n\\nif not recent_papers:\\n    print(\\\"No recent papers found on this topic.\\\")\\n```\\n\\nThis script fetches up to 20 papers with the query \\\"Large Language Models applications\\\" from the arXiv API, filters them to get the ones from the past week, and prints their title, authors, abstract, and link.\", \"role\": \"user\", \"name\": \"Engineer\"}]\n",
      "\u001b[32m\n",
      "Next speaker: Executor\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mExecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Paper 1:\n",
      "Title: Video-Panda: Parameter-efficient Alignment for Encoder-free\n",
      "  Video-Language Models\n",
      "Authors: Jinhui Yi\n",
      "Abstract: We present an efficient encoder-free approach for video-language\n",
      "understanding that achieves competitive performance while significantly\n",
      "reducing computational overhead. Current video-language models typically rely\n",
      "on heavyweight image encoders (300M-1.1B parameters) or video encoders (1B-1.4B\n",
      "parameters), creating a substantial computational burden when processing\n",
      "multi-frame videos. Our method introduces a novel Spatio-Temporal Alignment\n",
      "Block (STAB) that directly processes video inputs without requiring pre-trained\n",
      "encoders while using only 45M parameters for visual processing - at least a\n",
      "6.5$\\times$ reduction compared to traditional approaches. The STAB architecture\n",
      "combines Local Spatio-Temporal Encoding for fine-grained feature extraction,\n",
      "efficient spatial downsampling through learned attention and separate\n",
      "mechanisms for modeling frame-level and video-level relationships. Our model\n",
      "achieves comparable or superior performance to encoder-based approaches for\n",
      "open-ended video question answering on standard benchmarks. The fine-grained\n",
      "video question-answering evaluation demonstrates our model's effectiveness,\n",
      "outperforming the encoder-based approaches Video-ChatGPT and Video-LLaVA in key\n",
      "aspects like correctness and temporal understanding. Extensive ablation studies\n",
      "validate our architectural choices and demonstrate the effectiveness of our\n",
      "spatio-temporal modeling approach while achieving 3-4$\\times$ faster processing\n",
      "speeds than previous methods. Code is available at\n",
      "\\url{https://github.com/jh-yi/Video-Panda}.\n",
      "Link: http://arxiv.org/abs/2412.18609v1\n",
      "\n",
      "Paper 2:\n",
      "Title: PartGen: Part-level 3D Generation and Reconstruction with Multi-View\n",
      "  Diffusion Models\n",
      "Authors: Minghao Chen\n",
      "Abstract: Text- or image-to-3D generators and 3D scanners can now produce 3D assets\n",
      "with high-quality shapes and textures. These assets typically consist of a\n",
      "single, fused representation, like an implicit neural field, a Gaussian\n",
      "mixture, or a mesh, without any useful structure. However, most applications\n",
      "and creative workflows require assets to be made of several meaningful parts\n",
      "that can be manipulated independently. To address this gap, we introduce\n",
      "PartGen, a novel approach that generates 3D objects composed of meaningful\n",
      "parts starting from text, an image, or an unstructured 3D object. First, given\n",
      "multiple views of a 3D object, generated or rendered, a multi-view diffusion\n",
      "model extracts a set of plausible and view-consistent part segmentations,\n",
      "dividing the object into parts. Then, a second multi-view diffusion model takes\n",
      "each part separately, fills in the occlusions, and uses those completed views\n",
      "for 3D reconstruction by feeding them to a 3D reconstruction network. This\n",
      "completion process considers the context of the entire object to ensure that\n",
      "the parts integrate cohesively. The generative completion model can make up for\n",
      "the information missing due to occlusions; in extreme cases, it can hallucinate\n",
      "entirely invisible parts based on the input 3D asset. We evaluate our method on\n",
      "generated and real 3D assets and show that it outperforms segmentation and\n",
      "part-extraction baselines by a large margin. We also showcase downstream\n",
      "applications such as 3D part editing.\n",
      "Link: http://arxiv.org/abs/2412.18608v1\n",
      "\n",
      "Paper 3:\n",
      "Title: DrivingGPT: Unifying Driving World Modeling and Planning with\n",
      "  Multi-modal Autoregressive Transformers\n",
      "Authors: Yuntao Chen\n",
      "Abstract: World model-based searching and planning are widely recognized as a promising\n",
      "path toward human-level physical intelligence. However, current driving world\n",
      "models primarily rely on video diffusion models, which specialize in visual\n",
      "generation but lack the flexibility to incorporate other modalities like\n",
      "action. In contrast, autoregressive transformers have demonstrated exceptional\n",
      "capability in modeling multimodal data. Our work aims to unify both driving\n",
      "model simulation and trajectory planning into a single sequence modeling\n",
      "problem. We introduce a multimodal driving language based on interleaved image\n",
      "and action tokens, and develop DrivingGPT to learn joint world modeling and\n",
      "planning through standard next-token prediction. Our DrivingGPT demonstrates\n",
      "strong performance in both action-conditioned video generation and end-to-end\n",
      "planning, outperforming strong baselines on large-scale nuPlan and NAVSIM\n",
      "benchmarks.\n",
      "Link: http://arxiv.org/abs/2412.18607v1\n",
      "\n",
      "Paper 4:\n",
      "Title: Lattice T-duality from non-invertible symmetries in quantum spin chains\n",
      "Authors: Salvatore D. Pace\n",
      "Abstract: Dualities of quantum field theories are challenging to realize in lattice\n",
      "models of qubits. In this work, we explore one of the simplest dualities,\n",
      "T-duality of the compact boson CFT, and its realization in quantum spin chains.\n",
      "In the special case of the XX model, we uncover an exact lattice T-duality,\n",
      "which is associated with a non-invertible symmetry that exchanges two lattice\n",
      "U(1) symmetries. The latter symmetries flow to the momentum and winding U(1)\n",
      "symmetries with a mixed anomaly in the CFT. However, the charge operators of\n",
      "the two U(1) symmetries do not commute on the lattice and instead generate the\n",
      "Onsager algebra. We discuss how some of the anomalies in the CFT are\n",
      "nonetheless still exactly realized on the lattice and how the lattice U(1)\n",
      "symmetries enforce gaplessness. We further explore lattice deformations\n",
      "preserving both U(1) symmetries and find a rich gapless phase diagram with\n",
      "special $\\mathrm{Spin}(2k)_1$ WZW model points and whose phase transitions all\n",
      "have dynamical exponent ${z&gt;1}$.\n",
      "Link: http://arxiv.org/abs/2412.18606v1\n",
      "\n",
      "Paper 5:\n",
      "Title: Orient Anything: Learning Robust Object Orientation Estimation from\n",
      "  Rendering 3D Models\n",
      "Authors: Zehan Wang\n",
      "Abstract: Orientation is a key attribute of objects, crucial for understanding their\n",
      "spatial pose and arrangement in images. However, practical solutions for\n",
      "accurate orientation estimation from a single image remain underexplored. In\n",
      "this work, we introduce Orient Anything, the first expert and foundational\n",
      "model designed to estimate object orientation in a single- and free-view image.\n",
      "Due to the scarcity of labeled data, we propose extracting knowledge from the\n",
      "3D world. By developing a pipeline to annotate the front face of 3D objects and\n",
      "render images from random views, we collect 2M images with precise orientation\n",
      "annotations. To fully leverage the dataset, we design a robust training\n",
      "objective that models the 3D orientation as probability distributions of three\n",
      "angles and predicts the object orientation by fitting these distributions.\n",
      "Besides, we employ several strategies to improve synthetic-to-real transfer.\n",
      "Our model achieves state-of-the-art orientation estimation accuracy in both\n",
      "rendered and real images and exhibits impressive zero-shot ability in various\n",
      "scenarios. More importantly, our model enhances many applications, such as\n",
      "comprehension and generation of complex spatial concepts and 3D object pose\n",
      "adjustment.\n",
      "Link: http://arxiv.org/abs/2412.18605v1\n",
      "\n",
      "Paper 6:\n",
      "Title: Explaining in Diffusion: Explaining a Classifier Through Hierarchical\n",
      "  Semantics with Text-to-Image Diffusion Models\n",
      "Authors: Tahira Kazimi\n",
      "Abstract: Classifiers are important components in many computer vision tasks, serving\n",
      "as the foundational backbone of a wide variety of models employed across\n",
      "diverse applications. However, understanding the decision-making process of\n",
      "classifiers remains a significant challenge. We propose DiffEx, a novel method\n",
      "that leverages the capabilities of text-to-image diffusion models to explain\n",
      "classifier decisions. Unlike traditional GAN-based explainability models, which\n",
      "are limited to simple, single-concept analyses and typically require training a\n",
      "new model for each classifier, our approach can explain classifiers that focus\n",
      "on single concepts (such as faces or animals) as well as those that handle\n",
      "complex scenes involving multiple concepts. DiffEx employs vision-language\n",
      "models to create a hierarchical list of semantics, allowing users to identify\n",
      "not only the overarching semantic influences on classifiers (e.g., the 'beard'\n",
      "semantic in a facial classifier) but also their sub-types, such as 'goatee' or\n",
      "'Balbo' beard. Our experiments demonstrate that DiffEx is able to cover a\n",
      "significantly broader spectrum of semantics compared to its GAN counterparts,\n",
      "providing a hierarchical tool that delivers a more detailed and fine-grained\n",
      "understanding of classifier decisions.\n",
      "Link: http://arxiv.org/abs/2412.18604v1\n",
      "\n",
      "Paper 7:\n",
      "Title: Long-Form Speech Generation with Spoken Language Models\n",
      "Authors: Se Jin Park\n",
      "Abstract: We consider the generative modeling of speech over multiple minutes, a\n",
      "requirement for long-form multimedia generation and audio-native voice\n",
      "assistants. However, current spoken language models struggle to generate\n",
      "plausible speech past tens of seconds, from high temporal resolution of speech\n",
      "tokens causing loss of coherence, to architectural issues with long-sequence\n",
      "training or extrapolation, to memory costs at inference time. With these\n",
      "considerations we propose SpeechSSM, the first speech language model to learn\n",
      "from and sample long-form spoken audio (e.g., 16 minutes of read or\n",
      "extemporaneous speech) in a single decoding session without text intermediates,\n",
      "based on recent advances in linear-time sequence modeling. Furthermore, to\n",
      "address growing challenges in spoken language evaluation, especially in this\n",
      "new long-form setting, we propose: new embedding-based and LLM-judged metrics;\n",
      "quality measurements over length and time; and a new benchmark for long-form\n",
      "speech processing and generation, LibriSpeech-Long. Speech samples and the\n",
      "dataset are released at\n",
      "https://google.github.io/tacotron/publications/speechssm/\n",
      "Link: http://arxiv.org/abs/2412.18603v1\n",
      "\n",
      "Paper 8:\n",
      "Title: Decentralized Intelligence in GameFi: Embodied AI Agents and the\n",
      "  Convergence of DeFi and Virtual Ecosystems\n",
      "Authors: Fernando Jia\n",
      "Abstract: In the rapidly evolving landscape of GameFi, a fusion of gaming and\n",
      "decentralized finance (DeFi), there exists a critical need to enhance player\n",
      "engagement and economic interaction within gaming ecosystems. Our GameFi\n",
      "ecosystem aims to fundamentally transform this landscape by integrating\n",
      "advanced embodied AI agents into GameFi platforms. These AI agents, developed\n",
      "using cutting-edge large language models (LLMs), such as GPT-4 and Claude AI,\n",
      "are capable of proactive, adaptive, and contextually rich interactions with\n",
      "players. By going beyond traditional scripted responses, these agents become\n",
      "integral participants in the game's narrative and economic systems, directly\n",
      "influencing player strategies and in-game economies. We address the limitations\n",
      "of current GameFi platforms, which often lack immersive AI interactions and\n",
      "mechanisms for community engagement or creator monetization. Through the deep\n",
      "integration of AI agents with blockchain technology, we establish a\n",
      "consensus-driven, decentralized GameFi ecosystem. This ecosystem empowers\n",
      "creators to monetize their contributions and fosters democratic collaboration\n",
      "among players and creators. Furthermore, by embedding DeFi mechanisms into the\n",
      "gaming experience, we enhance economic participation and provide new\n",
      "opportunities for financial interactions within the game. Our approach enhances\n",
      "player immersion and retention and advances the GameFi ecosystem by bridging\n",
      "traditional gaming with Web3 technologies. By integrating sophisticated AI and\n",
      "DeFi elements, we contribute to the development of more engaging, economically\n",
      "robust, and community-centric gaming environments. This project represents a\n",
      "significant advancement in the state-of-the-art in GameFi, offering insights\n",
      "and methodologies that can be applied throughout the gaming industry.\n",
      "Link: http://arxiv.org/abs/2412.18601v1\n",
      "\n",
      "Paper 9:\n",
      "Title: ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation\n",
      "Authors: Hongjie Li\n",
      "Abstract: Human-scene interaction (HSI) generation is crucial for applications in\n",
      "embodied AI, virtual reality, and robotics. While existing methods can\n",
      "synthesize realistic human motions in 3D scenes and generate plausible\n",
      "human-object interactions, they heavily rely on datasets containing paired 3D\n",
      "scene and motion capture data, which are expensive and time-consuming to\n",
      "collect across diverse environments and interactions. We present ZeroHSI, a\n",
      "novel approach that enables zero-shot 4D human-scene interaction synthesis by\n",
      "integrating video generation and neural human rendering. Our key insight is to\n",
      "leverage the rich motion priors learned by state-of-the-art video generation\n",
      "models, which have been trained on vast amounts of natural human movements and\n",
      "interactions, and use differentiable rendering to reconstruct human-scene\n",
      "interactions. ZeroHSI can synthesize realistic human motions in both static\n",
      "scenes and environments with dynamic objects, without requiring any\n",
      "ground-truth motion data. We evaluate ZeroHSI on a curated dataset of different\n",
      "types of various indoor and outdoor scenes with different interaction prompts,\n",
      "demonstrating its ability to generate diverse and contextually appropriate\n",
      "human-scene interactions.\n",
      "Link: http://arxiv.org/abs/2412.18600v1\n",
      "\n",
      "Paper 10:\n",
      "Title: Double Spending Analysis of Nakamoto Consensus for Time-Varying Mining\n",
      "  Rates with Ruin Theory\n",
      "Authors: Mustafa Doger\n",
      "Abstract: Theoretical guarantees for double spending probabilities for the Nakamoto\n",
      "consensus under the $k$-deep confirmation rule have been extensively studied\n",
      "for zero/bounded network delays and fixed mining rates. In this paper, we\n",
      "introduce a ruin-theoretical model of double spending for Nakamoto consensus\n",
      "under the $k$-deep confirmation rule when the honest mining rate is allowed to\n",
      "be an arbitrary function of time including the block delivery periods, i.e.,\n",
      "time periods during which mined blocks are being delivered to all other\n",
      "participants of the network. Time-varying mining rates are considered to\n",
      "capture the intrinsic characteristics of the peer to peer network delays as\n",
      "well as dynamic participation of miners such as the gap game and switching\n",
      "between different cryptocurrencies. Ruin theory is leveraged to obtain the\n",
      "double spend probabilities and numerical examples are presented to validate the\n",
      "effectiveness of the proposed analytical method.\n",
      "Link: http://arxiv.org/abs/2412.18599v1\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Message: [{\"content\": \"Topic: LLM applications papers from last week. Requirement: 5 - 10 papers from different domains.\", \"role\": \"user\", \"name\": \"Admin\"}, {\"content\": \"Sure! I will write a Python script to retrieve papers from the arXiv API based on the topic \\\"Large Language Models (LLM) applications\\\" and filter them to get the latest papers from the past week. The script will display the title, authors, abstract, and link for each paper.\\n\\n```python\\nimport requests\\nimport datetime\\nfrom dateutil.parser import parse\\n\\ndef get_arxiv_papers(query, max_results=10):\\n    base_url = 'http://export.arxiv.org/api/query?'\\n    query = f'search_query=all:{query}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending'\\n    \\n    response = requests.get(base_url + query)\\n    \\n    if response.status_code != 200:\\n        print(\\\"Error fetching data from arXiv API\\\")\\n        return\\n    \\n    feed = response.text\\n    start_pos = 0\\n    papers = []\\n\\n    while True:\\n        start_index = feed.find('<entry>', start_pos)\\n        if start_index == -1:\\n            break\\n        end_index = feed.find('</entry>', start_index)\\n        entry = feed[start_index:end_index]\\n        \\n        title_start = entry.find('<title>') + len('<title>')\\n        title_end = entry.find('</title>', title_start)\\n        title = entry[title_start:title_end].strip()\\n        \\n        authors_start = entry.find('<author>')\\n        authors_end = entry.find('</author>', authors_start)\\n        authors = []\\n        while authors_start != -1 and authors_start < authors_end:\\n            author_start = entry.find('<name>', authors_start) + len('<name>')\\n            author_end = entry.find('</name>', author_start)\\n            authors.append(entry[author_start:author_end].strip())\\n            authors_start = entry.find('<author>', author_end)\\n        \\n        abstract_start = entry.find('<summary>') + len('<summary>')\\n        abstract_end = entry.find('</summary>', abstract_start)\\n        abstract = entry[abstract_start:abstract_end].strip()\\n\\n        link_start = entry.find('<id>') + len('<id>')\\n        link_end = entry.find('</id>', link_start)\\n        link = entry[link_start:link_end].strip()\\n\\n        updated_start = entry.find('<updated>') + len('<updated>')\\n        updated_end = entry.find('</updated>', updated_start)\\n        updated = entry[updated_start:updated_end].strip()\\n\\n        papers.append({\\n            'title': title,\\n            'authors': authors,\\n            'abstract': abstract,\\n            'link': link,\\n            'updated': parse(updated)\\n        })\\n\\n        start_pos = end_index\\n    \\n    return papers\\n\\ndef filter_recent_papers(papers, days=7):\\n    recent_papers = []\\n    current_time = datetime.datetime.now(datetime.timezone.utc)\\n    for paper in papers:\\n        if (current_time - paper['updated']).days <= days:\\n            recent_papers.append(paper)\\n    return recent_papers\\n\\nquery = \\\"Large Language Models applications\\\"\\npapers = get_arxiv_papers(query, max_results=20)\\nrecent_papers = filter_recent_papers(papers)\\n\\nfor idx, paper in enumerate(recent_papers[:10]):\\n    print(f\\\"Paper {idx + 1}:\\\")\\n    print(f\\\"Title: {paper['title']}\\\")\\n    print(f\\\"Authors: {', '.join(paper['authors'])}\\\")\\n    print(f\\\"Abstract: {paper['abstract']}\\\")\\n    print(f\\\"Link: {paper['link']}\\\")\\n    print()\\n\\nif not recent_papers:\\n    print(\\\"No recent papers found on this topic.\\\")\\n```\\n\\nThis script fetches up to 20 papers with the query \\\"Large Language Models applications\\\" from the arXiv API, filters them to get the ones from the past week, and prints their title, authors, abstract, and link.\", \"role\": \"user\", \"name\": \"Engineer\"}, {\"content\": \"exitcode: 0 (execution succeeded)\\nCode output: \\nPaper 1:\\nTitle: Video-Panda: Parameter-efficient Alignment for Encoder-free\\n  Video-Language Models\\nAuthors: Jinhui Yi\\nAbstract: We present an efficient encoder-free approach for video-language\\nunderstanding that achieves competitive performance while significantly\\nreducing computational overhead. Current video-language models typically rely\\non heavyweight image encoders (300M-1.1B parameters) or video encoders (1B-1.4B\\nparameters), creating a substantial computational burden when processing\\nmulti-frame videos. Our method introduces a novel Spatio-Temporal Alignment\\nBlock (STAB) that directly processes video inputs without requiring pre-trained\\nencoders while using only 45M parameters for visual processing - at least a\\n6.5$\\\\times$ reduction compared to traditional approaches. The STAB architecture\\ncombines Local Spatio-Temporal Encoding for fine-grained feature extraction,\\nefficient spatial downsampling through learned attention and separate\\nmechanisms for modeling frame-level and video-level relationships. Our model\\nachieves comparable or superior performance to encoder-based approaches for\\nopen-ended video question answering on standard benchmarks. The fine-grained\\nvideo question-answering evaluation demonstrates our model's effectiveness,\\noutperforming the encoder-based approaches Video-ChatGPT and Video-LLaVA in key\\naspects like correctness and temporal understanding. Extensive ablation studies\\nvalidate our architectural choices and demonstrate the effectiveness of our\\nspatio-temporal modeling approach while achieving 3-4$\\\\times$ faster processing\\nspeeds than previous methods. Code is available at\\n\\\\url{https://github.com/jh-yi/Video-Panda}.\\nLink: http://arxiv.org/abs/2412.18609v1\\n\\nPaper 2:\\nTitle: PartGen: Part-level 3D Generation and Reconstruction with Multi-View\\n  Diffusion Models\\nAuthors: Minghao Chen\\nAbstract: Text- or image-to-3D generators and 3D scanners can now produce 3D assets\\nwith high-quality shapes and textures. These assets typically consist of a\\nsingle, fused representation, like an implicit neural field, a Gaussian\\nmixture, or a mesh, without any useful structure. However, most applications\\nand creative workflows require assets to be made of several meaningful parts\\nthat can be manipulated independently. To address this gap, we introduce\\nPartGen, a novel approach that generates 3D objects composed of meaningful\\nparts starting from text, an image, or an unstructured 3D object. First, given\\nmultiple views of a 3D object, generated or rendered, a multi-view diffusion\\nmodel extracts a set of plausible and view-consistent part segmentations,\\ndividing the object into parts. Then, a second multi-view diffusion model takes\\neach part separately, fills in the occlusions, and uses those completed views\\nfor 3D reconstruction by feeding them to a 3D reconstruction network. This\\ncompletion process considers the context of the entire object to ensure that\\nthe parts integrate cohesively. The generative completion model can make up for\\nthe information missing due to occlusions; in extreme cases, it can hallucinate\\nentirely invisible parts based on the input 3D asset. We evaluate our method on\\ngenerated and real 3D assets and show that it outperforms segmentation and\\npart-extraction baselines by a large margin. We also showcase downstream\\napplications such as 3D part editing.\\nLink: http://arxiv.org/abs/2412.18608v1\\n\\nPaper 3:\\nTitle: DrivingGPT: Unifying Driving World Modeling and Planning with\\n  Multi-modal Autoregressive Transformers\\nAuthors: Yuntao Chen\\nAbstract: World model-based searching and planning are widely recognized as a promising\\npath toward human-level physical intelligence. However, current driving world\\nmodels primarily rely on video diffusion models, which specialize in visual\\ngeneration but lack the flexibility to incorporate other modalities like\\naction. In contrast, autoregressive transformers have demonstrated exceptional\\ncapability in modeling multimodal data. Our work aims to unify both driving\\nmodel simulation and trajectory planning into a single sequence modeling\\nproblem. We introduce a multimodal driving language based on interleaved image\\nand action tokens, and develop DrivingGPT to learn joint world modeling and\\nplanning through standard next-token prediction. Our DrivingGPT demonstrates\\nstrong performance in both action-conditioned video generation and end-to-end\\nplanning, outperforming strong baselines on large-scale nuPlan and NAVSIM\\nbenchmarks.\\nLink: http://arxiv.org/abs/2412.18607v1\\n\\nPaper 4:\\nTitle: Lattice T-duality from non-invertible symmetries in quantum spin chains\\nAuthors: Salvatore D. Pace\\nAbstract: Dualities of quantum field theories are challenging to realize in lattice\\nmodels of qubits. In this work, we explore one of the simplest dualities,\\nT-duality of the compact boson CFT, and its realization in quantum spin chains.\\nIn the special case of the XX model, we uncover an exact lattice T-duality,\\nwhich is associated with a non-invertible symmetry that exchanges two lattice\\nU(1) symmetries. The latter symmetries flow to the momentum and winding U(1)\\nsymmetries with a mixed anomaly in the CFT. However, the charge operators of\\nthe two U(1) symmetries do not commute on the lattice and instead generate the\\nOnsager algebra. We discuss how some of the anomalies in the CFT are\\nnonetheless still exactly realized on the lattice and how the lattice U(1)\\nsymmetries enforce gaplessness. We further explore lattice deformations\\npreserving both U(1) symmetries and find a rich gapless phase diagram with\\nspecial $\\\\mathrm{Spin}(2k)_1$ WZW model points and whose phase transitions all\\nhave dynamical exponent ${z&gt;1}$.\\nLink: http://arxiv.org/abs/2412.18606v1\\n\\nPaper 5:\\nTitle: Orient Anything: Learning Robust Object Orientation Estimation from\\n  Rendering 3D Models\\nAuthors: Zehan Wang\\nAbstract: Orientation is a key attribute of objects, crucial for understanding their\\nspatial pose and arrangement in images. However, practical solutions for\\naccurate orientation estimation from a single image remain underexplored. In\\nthis work, we introduce Orient Anything, the first expert and foundational\\nmodel designed to estimate object orientation in a single- and free-view image.\\nDue to the scarcity of labeled data, we propose extracting knowledge from the\\n3D world. By developing a pipeline to annotate the front face of 3D objects and\\nrender images from random views, we collect 2M images with precise orientation\\nannotations. To fully leverage the dataset, we design a robust training\\nobjective that models the 3D orientation as probability distributions of three\\nangles and predicts the object orientation by fitting these distributions.\\nBesides, we employ several strategies to improve synthetic-to-real transfer.\\nOur model achieves state-of-the-art orientation estimation accuracy in both\\nrendered and real images and exhibits impressive zero-shot ability in various\\nscenarios. More importantly, our model enhances many applications, such as\\ncomprehension and generation of complex spatial concepts and 3D object pose\\nadjustment.\\nLink: http://arxiv.org/abs/2412.18605v1\\n\\nPaper 6:\\nTitle: Explaining in Diffusion: Explaining a Classifier Through Hierarchical\\n  Semantics with Text-to-Image Diffusion Models\\nAuthors: Tahira Kazimi\\nAbstract: Classifiers are important components in many computer vision tasks, serving\\nas the foundational backbone of a wide variety of models employed across\\ndiverse applications. However, understanding the decision-making process of\\nclassifiers remains a significant challenge. We propose DiffEx, a novel method\\nthat leverages the capabilities of text-to-image diffusion models to explain\\nclassifier decisions. Unlike traditional GAN-based explainability models, which\\nare limited to simple, single-concept analyses and typically require training a\\nnew model for each classifier, our approach can explain classifiers that focus\\non single concepts (such as faces or animals) as well as those that handle\\ncomplex scenes involving multiple concepts. DiffEx employs vision-language\\nmodels to create a hierarchical list of semantics, allowing users to identify\\nnot only the overarching semantic influences on classifiers (e.g., the 'beard'\\nsemantic in a facial classifier) but also their sub-types, such as 'goatee' or\\n'Balbo' beard. Our experiments demonstrate that DiffEx is able to cover a\\nsignificantly broader spectrum of semantics compared to its GAN counterparts,\\nproviding a hierarchical tool that delivers a more detailed and fine-grained\\nunderstanding of classifier decisions.\\nLink: http://arxiv.org/abs/2412.18604v1\\n\\nPaper 7:\\nTitle: Long-Form Speech Generation with Spoken Language Models\\nAuthors: Se Jin Park\\nAbstract: We consider the generative modeling of speech over multiple minutes, a\\nrequirement for long-form multimedia generation and audio-native voice\\nassistants. However, current spoken language models struggle to generate\\nplausible speech past tens of seconds, from high temporal resolution of speech\\ntokens causing loss of coherence, to architectural issues with long-sequence\\ntraining or extrapolation, to memory costs at inference time. With these\\nconsiderations we propose SpeechSSM, the first speech language model to learn\\nfrom and sample long-form spoken audio (e.g., 16 minutes of read or\\nextemporaneous speech) in a single decoding session without text intermediates,\\nbased on recent advances in linear-time sequence modeling. Furthermore, to\\naddress growing challenges in spoken language evaluation, especially in this\\nnew long-form setting, we propose: new embedding-based and LLM-judged metrics;\\nquality measurements over length and time; and a new benchmark for long-form\\nspeech processing and generation, LibriSpeech-Long. Speech samples and the\\ndataset are released at\\nhttps://google.github.io/tacotron/publications/speechssm/\\nLink: http://arxiv.org/abs/2412.18603v1\\n\\nPaper 8:\\nTitle: Decentralized Intelligence in GameFi: Embodied AI Agents and the\\n  Convergence of DeFi and Virtual Ecosystems\\nAuthors: Fernando Jia\\nAbstract: In the rapidly evolving landscape of GameFi, a fusion of gaming and\\ndecentralized finance (DeFi), there exists a critical need to enhance player\\nengagement and economic interaction within gaming ecosystems. Our GameFi\\necosystem aims to fundamentally transform this landscape by integrating\\nadvanced embodied AI agents into GameFi platforms. These AI agents, developed\\nusing cutting-edge large language models (LLMs), such as GPT-4 and Claude AI,\\nare capable of proactive, adaptive, and contextually rich interactions with\\nplayers. By going beyond traditional scripted responses, these agents become\\nintegral participants in the game's narrative and economic systems, directly\\ninfluencing player strategies and in-game economies. We address the limitations\\nof current GameFi platforms, which often lack immersive AI interactions and\\nmechanisms for community engagement or creator monetization. Through the deep\\nintegration of AI agents with blockchain technology, we establish a\\nconsensus-driven, decentralized GameFi ecosystem. This ecosystem empowers\\ncreators to monetize their contributions and fosters democratic collaboration\\namong players and creators. Furthermore, by embedding DeFi mechanisms into the\\ngaming experience, we enhance economic participation and provide new\\nopportunities for financial interactions within the game. Our approach enhances\\nplayer immersion and retention and advances the GameFi ecosystem by bridging\\ntraditional gaming with Web3 technologies. By integrating sophisticated AI and\\nDeFi elements, we contribute to the development of more engaging, economically\\nrobust, and community-centric gaming environments. This project represents a\\nsignificant advancement in the state-of-the-art in GameFi, offering insights\\nand methodologies that can be applied throughout the gaming industry.\\nLink: http://arxiv.org/abs/2412.18601v1\\n\\nPaper 9:\\nTitle: ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation\\nAuthors: Hongjie Li\\nAbstract: Human-scene interaction (HSI) generation is crucial for applications in\\nembodied AI, virtual reality, and robotics. While existing methods can\\nsynthesize realistic human motions in 3D scenes and generate plausible\\nhuman-object interactions, they heavily rely on datasets containing paired 3D\\nscene and motion capture data, which are expensive and time-consuming to\\ncollect across diverse environments and interactions. We present ZeroHSI, a\\nnovel approach that enables zero-shot 4D human-scene interaction synthesis by\\nintegrating video generation and neural human rendering. Our key insight is to\\nleverage the rich motion priors learned by state-of-the-art video generation\\nmodels, which have been trained on vast amounts of natural human movements and\\ninteractions, and use differentiable rendering to reconstruct human-scene\\ninteractions. ZeroHSI can synthesize realistic human motions in both static\\nscenes and environments with dynamic objects, without requiring any\\nground-truth motion data. We evaluate ZeroHSI on a curated dataset of different\\ntypes of various indoor and outdoor scenes with different interaction prompts,\\ndemonstrating its ability to generate diverse and contextually appropriate\\nhuman-scene interactions.\\nLink: http://arxiv.org/abs/2412.18600v1\\n\\nPaper 10:\\nTitle: Double Spending Analysis of Nakamoto Consensus for Time-Varying Mining\\n  Rates with Ruin Theory\\nAuthors: Mustafa Doger\\nAbstract: Theoretical guarantees for double spending probabilities for the Nakamoto\\nconsensus under the $k$-deep confirmation rule have been extensively studied\\nfor zero/bounded network delays and fixed mining rates. In this paper, we\\nintroduce a ruin-theoretical model of double spending for Nakamoto consensus\\nunder the $k$-deep confirmation rule when the honest mining rate is allowed to\\nbe an arbitrary function of time including the block delivery periods, i.e.,\\ntime periods during which mined blocks are being delivered to all other\\nparticipants of the network. Time-varying mining rates are considered to\\ncapture the intrinsic characteristics of the peer to peer network delays as\\nwell as dynamic participation of miners such as the gap game and switching\\nbetween different cryptocurrencies. Ruin theory is leveraged to obtain the\\ndouble spend probabilities and numerical examples are presented to validate the\\neffectiveness of the proposed analytical method.\\nLink: http://arxiv.org/abs/2412.18599v1\\n\\n\", \"role\": \"user\", \"name\": \"Executor\"}]\n",
      "\u001b[32m\n",
      "Next speaker: Scientist\n",
      "\u001b[0m\n",
      "\u001b[33mScientist\u001b[0m (to chat_manager):\n",
      "\n",
      "Here is a markdown table categorizing the papers based on their domains:\n",
      "\n",
      "```markdown\n",
      "| Domain                               | Title                                                                                      | Authors                                   | Summary                                                                                                                                                                                                                                                                        | Link                                     |\n",
      "|--------------------------------------|--------------------------------------------------------------------------------------------|-------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------|\n",
      "| Video-Language Models                | Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models          | Jinhui Yi                                 | Introduces a Spatio-Temporal Alignment Block (STAB) for video-language understanding, reducing computational overhead significantly without using heavyweight encoders, achieving competitive performance for open-ended video question answering.                          | [Link](http://arxiv.org/abs/2412.18609v1) |\n",
      "| 3D Generation and Reconstruction     | PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models      | Minghao Chen                              | Proposes PartGen, a novel approach for generating 3D objects composed of meaningful parts from text, images, or unstructured 3D objects using multi-view diffusion models for part segmentation and reconstruction.                                                          | [Link](http://arxiv.org/abs/2412.18608v1) |\n",
      "| Autonomous Driving                   | DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers | Yuntao Chen                               | Introduces DrivingGPT, a multimodal autoregressive transformer for unified driving model simulation and trajectory planning, demonstrating strong performance in action-conditioned video generation and end-to-end planning.                                                   | [Link](http://arxiv.org/abs/2412.18607v1) |\n",
      "| Quantum Field Theories               | Lattice T-duality from non-invertible symmetries in quantum spin chains                     | Salvatore D. Pace                         | Explores T-duality of the compact boson CFT in quantum spin chains, uncovering exact lattice T-duality and non-invertible symmetries, and examining the gapless phase diagram with special WZW model points.                                                               | [Link](http://arxiv.org/abs/2412.18606v1) |\n",
      "| Object Orientation Estimation        | Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models     | Zehan Wang                                | Introduces Orient Anything, a foundational model for estimating object orientation from a single image, using a large dataset of rendered 3D objects to train robust orientation estimation models, achieving state-of-the-art accuracy.                                            | [Link](http://arxiv.org/abs/2412.18605v1) |\n",
      "| Explainable AI                       | Explaining in Diffusion: Explaining a Classifier Through Hierarchical Semantics with Text-to-Image Diffusion Models | Tahira Kazimi                             | Proposes DiffEx, which uses text-to-image diffusion models to explain classifier decisions, providing hierarchical and detailed explanations for complex scenes and multiple concepts, outperforming traditional GAN-based models.                                           | [Link](http://arxiv.org/abs/2412.18604v1) |\n",
      "| Speech Generation                    | Long-Form Speech Generation with Spoken Language Models                                    | Se Jin Park                               | Presents SpeechSSM, the first model to generate long-form spoken audio in a single session without text intermediates, addressing challenges in long-form speech generation and evaluation, and introducing new metrics and benchmarks.                                       | [Link](http://arxiv.org/abs/2412.18603v1) |\n",
      "| GameFi and Decentralized Intelligence| Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems | Fernando Jia                              | Integrates advanced AI agents into GameFi platforms using large language models, enhancing player engagement and economic interaction, and establishing decentralized, consensus-driven ecosystems for community collaboration and creator monetization.                    | [Link](http://arxiv.org/abs/2412.18601v1) |\n",
      "| Human-Scene Interaction              | ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation                           | Hongjie Li                                | Introduces ZeroHSI, enabling zero-shot 4D human-scene interaction synthesis by integrating video generation and neural human rendering, leveraging motion priors from state-of-the-art video generation models without requiring ground-truth motion data.                  | [Link](http://arxiv.org/abs/2412.18600v1) |\n",
      "| Blockchain and Ruin Theory           | Double Spending Analysis of Nakamoto Consensus for Time-Varying Mining Rates with Ruin Theory | Mustafa Doger                             | Introduces a ruin-theoretical model for double spending analysis in Nakamoto consensus under time-varying mining rates, considering network delays and dynamic miner participation, and presenting numerical examples to validate the proposed method.                           | [Link](http://arxiv.org/abs/2412.18599v1) |\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No eligible speaker found. Terminating the conversation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: [{\"content\": \"Topic: LLM applications papers from last week. Requirement: 5 - 10 papers from different domains.\", \"role\": \"user\", \"name\": \"Admin\"}, {\"content\": \"Sure! I will write a Python script to retrieve papers from the arXiv API based on the topic \\\"Large Language Models (LLM) applications\\\" and filter them to get the latest papers from the past week. The script will display the title, authors, abstract, and link for each paper.\\n\\n```python\\nimport requests\\nimport datetime\\nfrom dateutil.parser import parse\\n\\ndef get_arxiv_papers(query, max_results=10):\\n    base_url = 'http://export.arxiv.org/api/query?'\\n    query = f'search_query=all:{query}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending'\\n    \\n    response = requests.get(base_url + query)\\n    \\n    if response.status_code != 200:\\n        print(\\\"Error fetching data from arXiv API\\\")\\n        return\\n    \\n    feed = response.text\\n    start_pos = 0\\n    papers = []\\n\\n    while True:\\n        start_index = feed.find('<entry>', start_pos)\\n        if start_index == -1:\\n            break\\n        end_index = feed.find('</entry>', start_index)\\n        entry = feed[start_index:end_index]\\n        \\n        title_start = entry.find('<title>') + len('<title>')\\n        title_end = entry.find('</title>', title_start)\\n        title = entry[title_start:title_end].strip()\\n        \\n        authors_start = entry.find('<author>')\\n        authors_end = entry.find('</author>', authors_start)\\n        authors = []\\n        while authors_start != -1 and authors_start < authors_end:\\n            author_start = entry.find('<name>', authors_start) + len('<name>')\\n            author_end = entry.find('</name>', author_start)\\n            authors.append(entry[author_start:author_end].strip())\\n            authors_start = entry.find('<author>', author_end)\\n        \\n        abstract_start = entry.find('<summary>') + len('<summary>')\\n        abstract_end = entry.find('</summary>', abstract_start)\\n        abstract = entry[abstract_start:abstract_end].strip()\\n\\n        link_start = entry.find('<id>') + len('<id>')\\n        link_end = entry.find('</id>', link_start)\\n        link = entry[link_start:link_end].strip()\\n\\n        updated_start = entry.find('<updated>') + len('<updated>')\\n        updated_end = entry.find('</updated>', updated_start)\\n        updated = entry[updated_start:updated_end].strip()\\n\\n        papers.append({\\n            'title': title,\\n            'authors': authors,\\n            'abstract': abstract,\\n            'link': link,\\n            'updated': parse(updated)\\n        })\\n\\n        start_pos = end_index\\n    \\n    return papers\\n\\ndef filter_recent_papers(papers, days=7):\\n    recent_papers = []\\n    current_time = datetime.datetime.now(datetime.timezone.utc)\\n    for paper in papers:\\n        if (current_time - paper['updated']).days <= days:\\n            recent_papers.append(paper)\\n    return recent_papers\\n\\nquery = \\\"Large Language Models applications\\\"\\npapers = get_arxiv_papers(query, max_results=20)\\nrecent_papers = filter_recent_papers(papers)\\n\\nfor idx, paper in enumerate(recent_papers[:10]):\\n    print(f\\\"Paper {idx + 1}:\\\")\\n    print(f\\\"Title: {paper['title']}\\\")\\n    print(f\\\"Authors: {', '.join(paper['authors'])}\\\")\\n    print(f\\\"Abstract: {paper['abstract']}\\\")\\n    print(f\\\"Link: {paper['link']}\\\")\\n    print()\\n\\nif not recent_papers:\\n    print(\\\"No recent papers found on this topic.\\\")\\n```\\n\\nThis script fetches up to 20 papers with the query \\\"Large Language Models applications\\\" from the arXiv API, filters them to get the ones from the past week, and prints their title, authors, abstract, and link.\", \"role\": \"user\", \"name\": \"Engineer\"}, {\"content\": \"exitcode: 0 (execution succeeded)\\nCode output: \\nPaper 1:\\nTitle: Video-Panda: Parameter-efficient Alignment for Encoder-free\\n  Video-Language Models\\nAuthors: Jinhui Yi\\nAbstract: We present an efficient encoder-free approach for video-language\\nunderstanding that achieves competitive performance while significantly\\nreducing computational overhead. Current video-language models typically rely\\non heavyweight image encoders (300M-1.1B parameters) or video encoders (1B-1.4B\\nparameters), creating a substantial computational burden when processing\\nmulti-frame videos. Our method introduces a novel Spatio-Temporal Alignment\\nBlock (STAB) that directly processes video inputs without requiring pre-trained\\nencoders while using only 45M parameters for visual processing - at least a\\n6.5$\\\\times$ reduction compared to traditional approaches. The STAB architecture\\ncombines Local Spatio-Temporal Encoding for fine-grained feature extraction,\\nefficient spatial downsampling through learned attention and separate\\nmechanisms for modeling frame-level and video-level relationships. Our model\\nachieves comparable or superior performance to encoder-based approaches for\\nopen-ended video question answering on standard benchmarks. The fine-grained\\nvideo question-answering evaluation demonstrates our model's effectiveness,\\noutperforming the encoder-based approaches Video-ChatGPT and Video-LLaVA in key\\naspects like correctness and temporal understanding. Extensive ablation studies\\nvalidate our architectural choices and demonstrate the effectiveness of our\\nspatio-temporal modeling approach while achieving 3-4$\\\\times$ faster processing\\nspeeds than previous methods. Code is available at\\n\\\\url{https://github.com/jh-yi/Video-Panda}.\\nLink: http://arxiv.org/abs/2412.18609v1\\n\\nPaper 2:\\nTitle: PartGen: Part-level 3D Generation and Reconstruction with Multi-View\\n  Diffusion Models\\nAuthors: Minghao Chen\\nAbstract: Text- or image-to-3D generators and 3D scanners can now produce 3D assets\\nwith high-quality shapes and textures. These assets typically consist of a\\nsingle, fused representation, like an implicit neural field, a Gaussian\\nmixture, or a mesh, without any useful structure. However, most applications\\nand creative workflows require assets to be made of several meaningful parts\\nthat can be manipulated independently. To address this gap, we introduce\\nPartGen, a novel approach that generates 3D objects composed of meaningful\\nparts starting from text, an image, or an unstructured 3D object. First, given\\nmultiple views of a 3D object, generated or rendered, a multi-view diffusion\\nmodel extracts a set of plausible and view-consistent part segmentations,\\ndividing the object into parts. Then, a second multi-view diffusion model takes\\neach part separately, fills in the occlusions, and uses those completed views\\nfor 3D reconstruction by feeding them to a 3D reconstruction network. This\\ncompletion process considers the context of the entire object to ensure that\\nthe parts integrate cohesively. The generative completion model can make up for\\nthe information missing due to occlusions; in extreme cases, it can hallucinate\\nentirely invisible parts based on the input 3D asset. We evaluate our method on\\ngenerated and real 3D assets and show that it outperforms segmentation and\\npart-extraction baselines by a large margin. We also showcase downstream\\napplications such as 3D part editing.\\nLink: http://arxiv.org/abs/2412.18608v1\\n\\nPaper 3:\\nTitle: DrivingGPT: Unifying Driving World Modeling and Planning with\\n  Multi-modal Autoregressive Transformers\\nAuthors: Yuntao Chen\\nAbstract: World model-based searching and planning are widely recognized as a promising\\npath toward human-level physical intelligence. However, current driving world\\nmodels primarily rely on video diffusion models, which specialize in visual\\ngeneration but lack the flexibility to incorporate other modalities like\\naction. In contrast, autoregressive transformers have demonstrated exceptional\\ncapability in modeling multimodal data. Our work aims to unify both driving\\nmodel simulation and trajectory planning into a single sequence modeling\\nproblem. We introduce a multimodal driving language based on interleaved image\\nand action tokens, and develop DrivingGPT to learn joint world modeling and\\nplanning through standard next-token prediction. Our DrivingGPT demonstrates\\nstrong performance in both action-conditioned video generation and end-to-end\\nplanning, outperforming strong baselines on large-scale nuPlan and NAVSIM\\nbenchmarks.\\nLink: http://arxiv.org/abs/2412.18607v1\\n\\nPaper 4:\\nTitle: Lattice T-duality from non-invertible symmetries in quantum spin chains\\nAuthors: Salvatore D. Pace\\nAbstract: Dualities of quantum field theories are challenging to realize in lattice\\nmodels of qubits. In this work, we explore one of the simplest dualities,\\nT-duality of the compact boson CFT, and its realization in quantum spin chains.\\nIn the special case of the XX model, we uncover an exact lattice T-duality,\\nwhich is associated with a non-invertible symmetry that exchanges two lattice\\nU(1) symmetries. The latter symmetries flow to the momentum and winding U(1)\\nsymmetries with a mixed anomaly in the CFT. However, the charge operators of\\nthe two U(1) symmetries do not commute on the lattice and instead generate the\\nOnsager algebra. We discuss how some of the anomalies in the CFT are\\nnonetheless still exactly realized on the lattice and how the lattice U(1)\\nsymmetries enforce gaplessness. We further explore lattice deformations\\npreserving both U(1) symmetries and find a rich gapless phase diagram with\\nspecial $\\\\mathrm{Spin}(2k)_1$ WZW model points and whose phase transitions all\\nhave dynamical exponent ${z&gt;1}$.\\nLink: http://arxiv.org/abs/2412.18606v1\\n\\nPaper 5:\\nTitle: Orient Anything: Learning Robust Object Orientation Estimation from\\n  Rendering 3D Models\\nAuthors: Zehan Wang\\nAbstract: Orientation is a key attribute of objects, crucial for understanding their\\nspatial pose and arrangement in images. However, practical solutions for\\naccurate orientation estimation from a single image remain underexplored. In\\nthis work, we introduce Orient Anything, the first expert and foundational\\nmodel designed to estimate object orientation in a single- and free-view image.\\nDue to the scarcity of labeled data, we propose extracting knowledge from the\\n3D world. By developing a pipeline to annotate the front face of 3D objects and\\nrender images from random views, we collect 2M images with precise orientation\\nannotations. To fully leverage the dataset, we design a robust training\\nobjective that models the 3D orientation as probability distributions of three\\nangles and predicts the object orientation by fitting these distributions.\\nBesides, we employ several strategies to improve synthetic-to-real transfer.\\nOur model achieves state-of-the-art orientation estimation accuracy in both\\nrendered and real images and exhibits impressive zero-shot ability in various\\nscenarios. More importantly, our model enhances many applications, such as\\ncomprehension and generation of complex spatial concepts and 3D object pose\\nadjustment.\\nLink: http://arxiv.org/abs/2412.18605v1\\n\\nPaper 6:\\nTitle: Explaining in Diffusion: Explaining a Classifier Through Hierarchical\\n  Semantics with Text-to-Image Diffusion Models\\nAuthors: Tahira Kazimi\\nAbstract: Classifiers are important components in many computer vision tasks, serving\\nas the foundational backbone of a wide variety of models employed across\\ndiverse applications. However, understanding the decision-making process of\\nclassifiers remains a significant challenge. We propose DiffEx, a novel method\\nthat leverages the capabilities of text-to-image diffusion models to explain\\nclassifier decisions. Unlike traditional GAN-based explainability models, which\\nare limited to simple, single-concept analyses and typically require training a\\nnew model for each classifier, our approach can explain classifiers that focus\\non single concepts (such as faces or animals) as well as those that handle\\ncomplex scenes involving multiple concepts. DiffEx employs vision-language\\nmodels to create a hierarchical list of semantics, allowing users to identify\\nnot only the overarching semantic influences on classifiers (e.g., the 'beard'\\nsemantic in a facial classifier) but also their sub-types, such as 'goatee' or\\n'Balbo' beard. Our experiments demonstrate that DiffEx is able to cover a\\nsignificantly broader spectrum of semantics compared to its GAN counterparts,\\nproviding a hierarchical tool that delivers a more detailed and fine-grained\\nunderstanding of classifier decisions.\\nLink: http://arxiv.org/abs/2412.18604v1\\n\\nPaper 7:\\nTitle: Long-Form Speech Generation with Spoken Language Models\\nAuthors: Se Jin Park\\nAbstract: We consider the generative modeling of speech over multiple minutes, a\\nrequirement for long-form multimedia generation and audio-native voice\\nassistants. However, current spoken language models struggle to generate\\nplausible speech past tens of seconds, from high temporal resolution of speech\\ntokens causing loss of coherence, to architectural issues with long-sequence\\ntraining or extrapolation, to memory costs at inference time. With these\\nconsiderations we propose SpeechSSM, the first speech language model to learn\\nfrom and sample long-form spoken audio (e.g., 16 minutes of read or\\nextemporaneous speech) in a single decoding session without text intermediates,\\nbased on recent advances in linear-time sequence modeling. Furthermore, to\\naddress growing challenges in spoken language evaluation, especially in this\\nnew long-form setting, we propose: new embedding-based and LLM-judged metrics;\\nquality measurements over length and time; and a new benchmark for long-form\\nspeech processing and generation, LibriSpeech-Long. Speech samples and the\\ndataset are released at\\nhttps://google.github.io/tacotron/publications/speechssm/\\nLink: http://arxiv.org/abs/2412.18603v1\\n\\nPaper 8:\\nTitle: Decentralized Intelligence in GameFi: Embodied AI Agents and the\\n  Convergence of DeFi and Virtual Ecosystems\\nAuthors: Fernando Jia\\nAbstract: In the rapidly evolving landscape of GameFi, a fusion of gaming and\\ndecentralized finance (DeFi), there exists a critical need to enhance player\\nengagement and economic interaction within gaming ecosystems. Our GameFi\\necosystem aims to fundamentally transform this landscape by integrating\\nadvanced embodied AI agents into GameFi platforms. These AI agents, developed\\nusing cutting-edge large language models (LLMs), such as GPT-4 and Claude AI,\\nare capable of proactive, adaptive, and contextually rich interactions with\\nplayers. By going beyond traditional scripted responses, these agents become\\nintegral participants in the game's narrative and economic systems, directly\\ninfluencing player strategies and in-game economies. We address the limitations\\nof current GameFi platforms, which often lack immersive AI interactions and\\nmechanisms for community engagement or creator monetization. Through the deep\\nintegration of AI agents with blockchain technology, we establish a\\nconsensus-driven, decentralized GameFi ecosystem. This ecosystem empowers\\ncreators to monetize their contributions and fosters democratic collaboration\\namong players and creators. Furthermore, by embedding DeFi mechanisms into the\\ngaming experience, we enhance economic participation and provide new\\nopportunities for financial interactions within the game. Our approach enhances\\nplayer immersion and retention and advances the GameFi ecosystem by bridging\\ntraditional gaming with Web3 technologies. By integrating sophisticated AI and\\nDeFi elements, we contribute to the development of more engaging, economically\\nrobust, and community-centric gaming environments. This project represents a\\nsignificant advancement in the state-of-the-art in GameFi, offering insights\\nand methodologies that can be applied throughout the gaming industry.\\nLink: http://arxiv.org/abs/2412.18601v1\\n\\nPaper 9:\\nTitle: ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation\\nAuthors: Hongjie Li\\nAbstract: Human-scene interaction (HSI) generation is crucial for applications in\\nembodied AI, virtual reality, and robotics. While existing methods can\\nsynthesize realistic human motions in 3D scenes and generate plausible\\nhuman-object interactions, they heavily rely on datasets containing paired 3D\\nscene and motion capture data, which are expensive and time-consuming to\\ncollect across diverse environments and interactions. We present ZeroHSI, a\\nnovel approach that enables zero-shot 4D human-scene interaction synthesis by\\nintegrating video generation and neural human rendering. Our key insight is to\\nleverage the rich motion priors learned by state-of-the-art video generation\\nmodels, which have been trained on vast amounts of natural human movements and\\ninteractions, and use differentiable rendering to reconstruct human-scene\\ninteractions. ZeroHSI can synthesize realistic human motions in both static\\nscenes and environments with dynamic objects, without requiring any\\nground-truth motion data. We evaluate ZeroHSI on a curated dataset of different\\ntypes of various indoor and outdoor scenes with different interaction prompts,\\ndemonstrating its ability to generate diverse and contextually appropriate\\nhuman-scene interactions.\\nLink: http://arxiv.org/abs/2412.18600v1\\n\\nPaper 10:\\nTitle: Double Spending Analysis of Nakamoto Consensus for Time-Varying Mining\\n  Rates with Ruin Theory\\nAuthors: Mustafa Doger\\nAbstract: Theoretical guarantees for double spending probabilities for the Nakamoto\\nconsensus under the $k$-deep confirmation rule have been extensively studied\\nfor zero/bounded network delays and fixed mining rates. In this paper, we\\nintroduce a ruin-theoretical model of double spending for Nakamoto consensus\\nunder the $k$-deep confirmation rule when the honest mining rate is allowed to\\nbe an arbitrary function of time including the block delivery periods, i.e.,\\ntime periods during which mined blocks are being delivered to all other\\nparticipants of the network. Time-varying mining rates are considered to\\ncapture the intrinsic characteristics of the peer to peer network delays as\\nwell as dynamic participation of miners such as the gap game and switching\\nbetween different cryptocurrencies. Ruin theory is leveraged to obtain the\\ndouble spend probabilities and numerical examples are presented to validate the\\neffectiveness of the proposed analytical method.\\nLink: http://arxiv.org/abs/2412.18599v1\\n\\n\", \"role\": \"user\", \"name\": \"Executor\"}, {\"content\": \"Here is a markdown table categorizing the papers based on their domains:\\n\\n```markdown\\n| Domain                               | Title                                                                                      | Authors                                   | Summary                                                                                                                                                                                                                                                                        | Link                                     |\\n|--------------------------------------|--------------------------------------------------------------------------------------------|-------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------|\\n| Video-Language Models                | Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models          | Jinhui Yi                                 | Introduces a Spatio-Temporal Alignment Block (STAB) for video-language understanding, reducing computational overhead significantly without using heavyweight encoders, achieving competitive performance for open-ended video question answering.                          | [Link](http://arxiv.org/abs/2412.18609v1) |\\n| 3D Generation and Reconstruction     | PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models      | Minghao Chen                              | Proposes PartGen, a novel approach for generating 3D objects composed of meaningful parts from text, images, or unstructured 3D objects using multi-view diffusion models for part segmentation and reconstruction.                                                          | [Link](http://arxiv.org/abs/2412.18608v1) |\\n| Autonomous Driving                   | DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers | Yuntao Chen                               | Introduces DrivingGPT, a multimodal autoregressive transformer for unified driving model simulation and trajectory planning, demonstrating strong performance in action-conditioned video generation and end-to-end planning.                                                   | [Link](http://arxiv.org/abs/2412.18607v1) |\\n| Quantum Field Theories               | Lattice T-duality from non-invertible symmetries in quantum spin chains                     | Salvatore D. Pace                         | Explores T-duality of the compact boson CFT in quantum spin chains, uncovering exact lattice T-duality and non-invertible symmetries, and examining the gapless phase diagram with special WZW model points.                                                               | [Link](http://arxiv.org/abs/2412.18606v1) |\\n| Object Orientation Estimation        | Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models     | Zehan Wang                                | Introduces Orient Anything, a foundational model for estimating object orientation from a single image, using a large dataset of rendered 3D objects to train robust orientation estimation models, achieving state-of-the-art accuracy.                                            | [Link](http://arxiv.org/abs/2412.18605v1) |\\n| Explainable AI                       | Explaining in Diffusion: Explaining a Classifier Through Hierarchical Semantics with Text-to-Image Diffusion Models | Tahira Kazimi                             | Proposes DiffEx, which uses text-to-image diffusion models to explain classifier decisions, providing hierarchical and detailed explanations for complex scenes and multiple concepts, outperforming traditional GAN-based models.                                           | [Link](http://arxiv.org/abs/2412.18604v1) |\\n| Speech Generation                    | Long-Form Speech Generation with Spoken Language Models                                    | Se Jin Park                               | Presents SpeechSSM, the first model to generate long-form spoken audio in a single session without text intermediates, addressing challenges in long-form speech generation and evaluation, and introducing new metrics and benchmarks.                                       | [Link](http://arxiv.org/abs/2412.18603v1) |\\n| GameFi and Decentralized Intelligence| Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems | Fernando Jia                              | Integrates advanced AI agents into GameFi platforms using large language models, enhancing player engagement and economic interaction, and establishing decentralized, consensus-driven ecosystems for community collaboration and creator monetization.                    | [Link](http://arxiv.org/abs/2412.18601v1) |\\n| Human-Scene Interaction              | ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation                           | Hongjie Li                                | Introduces ZeroHSI, enabling zero-shot 4D human-scene interaction synthesis by integrating video generation and neural human rendering, leveraging motion priors from state-of-the-art video generation models without requiring ground-truth motion data.                  | [Link](http://arxiv.org/abs/2412.18600v1) |\\n| Blockchain and Ruin Theory           | Double Spending Analysis of Nakamoto Consensus for Time-Varying Mining Rates with Ruin Theory | Mustafa Doger                             | Introduces a ruin-theoretical model for double spending analysis in Nakamoto consensus under time-varying mining rates, considering network delays and dynamic miner participation, and presenting numerical examples to validate the proposed method.                           | [Link](http://arxiv.org/abs/2412.18599v1) |\\n```\", \"role\": \"user\", \"name\": \"Scientist\"}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'Topic: LLM applications papers from last week. Requirement: 5 - 10 papers from different domains.', 'role': 'assistant', 'name': 'Admin'}, {'content': 'Sure! I will write a Python script to retrieve papers from the arXiv API based on the topic \"Large Language Models (LLM) applications\" and filter them to get the latest papers from the past week. The script will display the title, authors, abstract, and link for each paper.\\n\\n```python\\nimport requests\\nimport datetime\\nfrom dateutil.parser import parse\\n\\ndef get_arxiv_papers(query, max_results=10):\\n    base_url = \\'http://export.arxiv.org/api/query?\\'\\n    query = f\\'search_query=all:{query}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending\\'\\n    \\n    response = requests.get(base_url + query)\\n    \\n    if response.status_code != 200:\\n        print(\"Error fetching data from arXiv API\")\\n        return\\n    \\n    feed = response.text\\n    start_pos = 0\\n    papers = []\\n\\n    while True:\\n        start_index = feed.find(\\'<entry>\\', start_pos)\\n        if start_index == -1:\\n            break\\n        end_index = feed.find(\\'</entry>\\', start_index)\\n        entry = feed[start_index:end_index]\\n        \\n        title_start = entry.find(\\'<title>\\') + len(\\'<title>\\')\\n        title_end = entry.find(\\'</title>\\', title_start)\\n        title = entry[title_start:title_end].strip()\\n        \\n        authors_start = entry.find(\\'<author>\\')\\n        authors_end = entry.find(\\'</author>\\', authors_start)\\n        authors = []\\n        while authors_start != -1 and authors_start < authors_end:\\n            author_start = entry.find(\\'<name>\\', authors_start) + len(\\'<name>\\')\\n            author_end = entry.find(\\'</name>\\', author_start)\\n            authors.append(entry[author_start:author_end].strip())\\n            authors_start = entry.find(\\'<author>\\', author_end)\\n        \\n        abstract_start = entry.find(\\'<summary>\\') + len(\\'<summary>\\')\\n        abstract_end = entry.find(\\'</summary>\\', abstract_start)\\n        abstract = entry[abstract_start:abstract_end].strip()\\n\\n        link_start = entry.find(\\'<id>\\') + len(\\'<id>\\')\\n        link_end = entry.find(\\'</id>\\', link_start)\\n        link = entry[link_start:link_end].strip()\\n\\n        updated_start = entry.find(\\'<updated>\\') + len(\\'<updated>\\')\\n        updated_end = entry.find(\\'</updated>\\', updated_start)\\n        updated = entry[updated_start:updated_end].strip()\\n\\n        papers.append({\\n            \\'title\\': title,\\n            \\'authors\\': authors,\\n            \\'abstract\\': abstract,\\n            \\'link\\': link,\\n            \\'updated\\': parse(updated)\\n        })\\n\\n        start_pos = end_index\\n    \\n    return papers\\n\\ndef filter_recent_papers(papers, days=7):\\n    recent_papers = []\\n    current_time = datetime.datetime.now(datetime.timezone.utc)\\n    for paper in papers:\\n        if (current_time - paper[\\'updated\\']).days <= days:\\n            recent_papers.append(paper)\\n    return recent_papers\\n\\nquery = \"Large Language Models applications\"\\npapers = get_arxiv_papers(query, max_results=20)\\nrecent_papers = filter_recent_papers(papers)\\n\\nfor idx, paper in enumerate(recent_papers[:10]):\\n    print(f\"Paper {idx + 1}:\")\\n    print(f\"Title: {paper[\\'title\\']}\")\\n    print(f\"Authors: {\\', \\'.join(paper[\\'authors\\'])}\")\\n    print(f\"Abstract: {paper[\\'abstract\\']}\")\\n    print(f\"Link: {paper[\\'link\\']}\")\\n    print()\\n\\nif not recent_papers:\\n    print(\"No recent papers found on this topic.\")\\n```\\n\\nThis script fetches up to 20 papers with the query \"Large Language Models applications\" from the arXiv API, filters them to get the ones from the past week, and prints their title, authors, abstract, and link.', 'name': 'Engineer', 'role': 'user'}, {'content': \"exitcode: 0 (execution succeeded)\\nCode output: \\nPaper 1:\\nTitle: Video-Panda: Parameter-efficient Alignment for Encoder-free\\n  Video-Language Models\\nAuthors: Jinhui Yi\\nAbstract: We present an efficient encoder-free approach for video-language\\nunderstanding that achieves competitive performance while significantly\\nreducing computational overhead. Current video-language models typically rely\\non heavyweight image encoders (300M-1.1B parameters) or video encoders (1B-1.4B\\nparameters), creating a substantial computational burden when processing\\nmulti-frame videos. Our method introduces a novel Spatio-Temporal Alignment\\nBlock (STAB) that directly processes video inputs without requiring pre-trained\\nencoders while using only 45M parameters for visual processing - at least a\\n6.5$\\\\times$ reduction compared to traditional approaches. The STAB architecture\\ncombines Local Spatio-Temporal Encoding for fine-grained feature extraction,\\nefficient spatial downsampling through learned attention and separate\\nmechanisms for modeling frame-level and video-level relationships. Our model\\nachieves comparable or superior performance to encoder-based approaches for\\nopen-ended video question answering on standard benchmarks. The fine-grained\\nvideo question-answering evaluation demonstrates our model's effectiveness,\\noutperforming the encoder-based approaches Video-ChatGPT and Video-LLaVA in key\\naspects like correctness and temporal understanding. Extensive ablation studies\\nvalidate our architectural choices and demonstrate the effectiveness of our\\nspatio-temporal modeling approach while achieving 3-4$\\\\times$ faster processing\\nspeeds than previous methods. Code is available at\\n\\\\url{https://github.com/jh-yi/Video-Panda}.\\nLink: http://arxiv.org/abs/2412.18609v1\\n\\nPaper 2:\\nTitle: PartGen: Part-level 3D Generation and Reconstruction with Multi-View\\n  Diffusion Models\\nAuthors: Minghao Chen\\nAbstract: Text- or image-to-3D generators and 3D scanners can now produce 3D assets\\nwith high-quality shapes and textures. These assets typically consist of a\\nsingle, fused representation, like an implicit neural field, a Gaussian\\nmixture, or a mesh, without any useful structure. However, most applications\\nand creative workflows require assets to be made of several meaningful parts\\nthat can be manipulated independently. To address this gap, we introduce\\nPartGen, a novel approach that generates 3D objects composed of meaningful\\nparts starting from text, an image, or an unstructured 3D object. First, given\\nmultiple views of a 3D object, generated or rendered, a multi-view diffusion\\nmodel extracts a set of plausible and view-consistent part segmentations,\\ndividing the object into parts. Then, a second multi-view diffusion model takes\\neach part separately, fills in the occlusions, and uses those completed views\\nfor 3D reconstruction by feeding them to a 3D reconstruction network. This\\ncompletion process considers the context of the entire object to ensure that\\nthe parts integrate cohesively. The generative completion model can make up for\\nthe information missing due to occlusions; in extreme cases, it can hallucinate\\nentirely invisible parts based on the input 3D asset. We evaluate our method on\\ngenerated and real 3D assets and show that it outperforms segmentation and\\npart-extraction baselines by a large margin. We also showcase downstream\\napplications such as 3D part editing.\\nLink: http://arxiv.org/abs/2412.18608v1\\n\\nPaper 3:\\nTitle: DrivingGPT: Unifying Driving World Modeling and Planning with\\n  Multi-modal Autoregressive Transformers\\nAuthors: Yuntao Chen\\nAbstract: World model-based searching and planning are widely recognized as a promising\\npath toward human-level physical intelligence. However, current driving world\\nmodels primarily rely on video diffusion models, which specialize in visual\\ngeneration but lack the flexibility to incorporate other modalities like\\naction. In contrast, autoregressive transformers have demonstrated exceptional\\ncapability in modeling multimodal data. Our work aims to unify both driving\\nmodel simulation and trajectory planning into a single sequence modeling\\nproblem. We introduce a multimodal driving language based on interleaved image\\nand action tokens, and develop DrivingGPT to learn joint world modeling and\\nplanning through standard next-token prediction. Our DrivingGPT demonstrates\\nstrong performance in both action-conditioned video generation and end-to-end\\nplanning, outperforming strong baselines on large-scale nuPlan and NAVSIM\\nbenchmarks.\\nLink: http://arxiv.org/abs/2412.18607v1\\n\\nPaper 4:\\nTitle: Lattice T-duality from non-invertible symmetries in quantum spin chains\\nAuthors: Salvatore D. Pace\\nAbstract: Dualities of quantum field theories are challenging to realize in lattice\\nmodels of qubits. In this work, we explore one of the simplest dualities,\\nT-duality of the compact boson CFT, and its realization in quantum spin chains.\\nIn the special case of the XX model, we uncover an exact lattice T-duality,\\nwhich is associated with a non-invertible symmetry that exchanges two lattice\\nU(1) symmetries. The latter symmetries flow to the momentum and winding U(1)\\nsymmetries with a mixed anomaly in the CFT. However, the charge operators of\\nthe two U(1) symmetries do not commute on the lattice and instead generate the\\nOnsager algebra. We discuss how some of the anomalies in the CFT are\\nnonetheless still exactly realized on the lattice and how the lattice U(1)\\nsymmetries enforce gaplessness. We further explore lattice deformations\\npreserving both U(1) symmetries and find a rich gapless phase diagram with\\nspecial $\\\\mathrm{Spin}(2k)_1$ WZW model points and whose phase transitions all\\nhave dynamical exponent ${z&gt;1}$.\\nLink: http://arxiv.org/abs/2412.18606v1\\n\\nPaper 5:\\nTitle: Orient Anything: Learning Robust Object Orientation Estimation from\\n  Rendering 3D Models\\nAuthors: Zehan Wang\\nAbstract: Orientation is a key attribute of objects, crucial for understanding their\\nspatial pose and arrangement in images. However, practical solutions for\\naccurate orientation estimation from a single image remain underexplored. In\\nthis work, we introduce Orient Anything, the first expert and foundational\\nmodel designed to estimate object orientation in a single- and free-view image.\\nDue to the scarcity of labeled data, we propose extracting knowledge from the\\n3D world. By developing a pipeline to annotate the front face of 3D objects and\\nrender images from random views, we collect 2M images with precise orientation\\nannotations. To fully leverage the dataset, we design a robust training\\nobjective that models the 3D orientation as probability distributions of three\\nangles and predicts the object orientation by fitting these distributions.\\nBesides, we employ several strategies to improve synthetic-to-real transfer.\\nOur model achieves state-of-the-art orientation estimation accuracy in both\\nrendered and real images and exhibits impressive zero-shot ability in various\\nscenarios. More importantly, our model enhances many applications, such as\\ncomprehension and generation of complex spatial concepts and 3D object pose\\nadjustment.\\nLink: http://arxiv.org/abs/2412.18605v1\\n\\nPaper 6:\\nTitle: Explaining in Diffusion: Explaining a Classifier Through Hierarchical\\n  Semantics with Text-to-Image Diffusion Models\\nAuthors: Tahira Kazimi\\nAbstract: Classifiers are important components in many computer vision tasks, serving\\nas the foundational backbone of a wide variety of models employed across\\ndiverse applications. However, understanding the decision-making process of\\nclassifiers remains a significant challenge. We propose DiffEx, a novel method\\nthat leverages the capabilities of text-to-image diffusion models to explain\\nclassifier decisions. Unlike traditional GAN-based explainability models, which\\nare limited to simple, single-concept analyses and typically require training a\\nnew model for each classifier, our approach can explain classifiers that focus\\non single concepts (such as faces or animals) as well as those that handle\\ncomplex scenes involving multiple concepts. DiffEx employs vision-language\\nmodels to create a hierarchical list of semantics, allowing users to identify\\nnot only the overarching semantic influences on classifiers (e.g., the 'beard'\\nsemantic in a facial classifier) but also their sub-types, such as 'goatee' or\\n'Balbo' beard. Our experiments demonstrate that DiffEx is able to cover a\\nsignificantly broader spectrum of semantics compared to its GAN counterparts,\\nproviding a hierarchical tool that delivers a more detailed and fine-grained\\nunderstanding of classifier decisions.\\nLink: http://arxiv.org/abs/2412.18604v1\\n\\nPaper 7:\\nTitle: Long-Form Speech Generation with Spoken Language Models\\nAuthors: Se Jin Park\\nAbstract: We consider the generative modeling of speech over multiple minutes, a\\nrequirement for long-form multimedia generation and audio-native voice\\nassistants. However, current spoken language models struggle to generate\\nplausible speech past tens of seconds, from high temporal resolution of speech\\ntokens causing loss of coherence, to architectural issues with long-sequence\\ntraining or extrapolation, to memory costs at inference time. With these\\nconsiderations we propose SpeechSSM, the first speech language model to learn\\nfrom and sample long-form spoken audio (e.g., 16 minutes of read or\\nextemporaneous speech) in a single decoding session without text intermediates,\\nbased on recent advances in linear-time sequence modeling. Furthermore, to\\naddress growing challenges in spoken language evaluation, especially in this\\nnew long-form setting, we propose: new embedding-based and LLM-judged metrics;\\nquality measurements over length and time; and a new benchmark for long-form\\nspeech processing and generation, LibriSpeech-Long. Speech samples and the\\ndataset are released at\\nhttps://google.github.io/tacotron/publications/speechssm/\\nLink: http://arxiv.org/abs/2412.18603v1\\n\\nPaper 8:\\nTitle: Decentralized Intelligence in GameFi: Embodied AI Agents and the\\n  Convergence of DeFi and Virtual Ecosystems\\nAuthors: Fernando Jia\\nAbstract: In the rapidly evolving landscape of GameFi, a fusion of gaming and\\ndecentralized finance (DeFi), there exists a critical need to enhance player\\nengagement and economic interaction within gaming ecosystems. Our GameFi\\necosystem aims to fundamentally transform this landscape by integrating\\nadvanced embodied AI agents into GameFi platforms. These AI agents, developed\\nusing cutting-edge large language models (LLMs), such as GPT-4 and Claude AI,\\nare capable of proactive, adaptive, and contextually rich interactions with\\nplayers. By going beyond traditional scripted responses, these agents become\\nintegral participants in the game's narrative and economic systems, directly\\ninfluencing player strategies and in-game economies. We address the limitations\\nof current GameFi platforms, which often lack immersive AI interactions and\\nmechanisms for community engagement or creator monetization. Through the deep\\nintegration of AI agents with blockchain technology, we establish a\\nconsensus-driven, decentralized GameFi ecosystem. This ecosystem empowers\\ncreators to monetize their contributions and fosters democratic collaboration\\namong players and creators. Furthermore, by embedding DeFi mechanisms into the\\ngaming experience, we enhance economic participation and provide new\\nopportunities for financial interactions within the game. Our approach enhances\\nplayer immersion and retention and advances the GameFi ecosystem by bridging\\ntraditional gaming with Web3 technologies. By integrating sophisticated AI and\\nDeFi elements, we contribute to the development of more engaging, economically\\nrobust, and community-centric gaming environments. This project represents a\\nsignificant advancement in the state-of-the-art in GameFi, offering insights\\nand methodologies that can be applied throughout the gaming industry.\\nLink: http://arxiv.org/abs/2412.18601v1\\n\\nPaper 9:\\nTitle: ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation\\nAuthors: Hongjie Li\\nAbstract: Human-scene interaction (HSI) generation is crucial for applications in\\nembodied AI, virtual reality, and robotics. While existing methods can\\nsynthesize realistic human motions in 3D scenes and generate plausible\\nhuman-object interactions, they heavily rely on datasets containing paired 3D\\nscene and motion capture data, which are expensive and time-consuming to\\ncollect across diverse environments and interactions. We present ZeroHSI, a\\nnovel approach that enables zero-shot 4D human-scene interaction synthesis by\\nintegrating video generation and neural human rendering. Our key insight is to\\nleverage the rich motion priors learned by state-of-the-art video generation\\nmodels, which have been trained on vast amounts of natural human movements and\\ninteractions, and use differentiable rendering to reconstruct human-scene\\ninteractions. ZeroHSI can synthesize realistic human motions in both static\\nscenes and environments with dynamic objects, without requiring any\\nground-truth motion data. We evaluate ZeroHSI on a curated dataset of different\\ntypes of various indoor and outdoor scenes with different interaction prompts,\\ndemonstrating its ability to generate diverse and contextually appropriate\\nhuman-scene interactions.\\nLink: http://arxiv.org/abs/2412.18600v1\\n\\nPaper 10:\\nTitle: Double Spending Analysis of Nakamoto Consensus for Time-Varying Mining\\n  Rates with Ruin Theory\\nAuthors: Mustafa Doger\\nAbstract: Theoretical guarantees for double spending probabilities for the Nakamoto\\nconsensus under the $k$-deep confirmation rule have been extensively studied\\nfor zero/bounded network delays and fixed mining rates. In this paper, we\\nintroduce a ruin-theoretical model of double spending for Nakamoto consensus\\nunder the $k$-deep confirmation rule when the honest mining rate is allowed to\\nbe an arbitrary function of time including the block delivery periods, i.e.,\\ntime periods during which mined blocks are being delivered to all other\\nparticipants of the network. Time-varying mining rates are considered to\\ncapture the intrinsic characteristics of the peer to peer network delays as\\nwell as dynamic participation of miners such as the gap game and switching\\nbetween different cryptocurrencies. Ruin theory is leveraged to obtain the\\ndouble spend probabilities and numerical examples are presented to validate the\\neffectiveness of the proposed analytical method.\\nLink: http://arxiv.org/abs/2412.18599v1\\n\\n\", 'name': 'Executor', 'role': 'user'}, {'content': 'Here is a markdown table categorizing the papers based on their domains:\\n\\n```markdown\\n| Domain                               | Title                                                                                      | Authors                                   | Summary                                                                                                                                                                                                                                                                        | Link                                     |\\n|--------------------------------------|--------------------------------------------------------------------------------------------|-------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------|\\n| Video-Language Models                | Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models          | Jinhui Yi                                 | Introduces a Spatio-Temporal Alignment Block (STAB) for video-language understanding, reducing computational overhead significantly without using heavyweight encoders, achieving competitive performance for open-ended video question answering.                          | [Link](http://arxiv.org/abs/2412.18609v1) |\\n| 3D Generation and Reconstruction     | PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models      | Minghao Chen                              | Proposes PartGen, a novel approach for generating 3D objects composed of meaningful parts from text, images, or unstructured 3D objects using multi-view diffusion models for part segmentation and reconstruction.                                                          | [Link](http://arxiv.org/abs/2412.18608v1) |\\n| Autonomous Driving                   | DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers | Yuntao Chen                               | Introduces DrivingGPT, a multimodal autoregressive transformer for unified driving model simulation and trajectory planning, demonstrating strong performance in action-conditioned video generation and end-to-end planning.                                                   | [Link](http://arxiv.org/abs/2412.18607v1) |\\n| Quantum Field Theories               | Lattice T-duality from non-invertible symmetries in quantum spin chains                     | Salvatore D. Pace                         | Explores T-duality of the compact boson CFT in quantum spin chains, uncovering exact lattice T-duality and non-invertible symmetries, and examining the gapless phase diagram with special WZW model points.                                                               | [Link](http://arxiv.org/abs/2412.18606v1) |\\n| Object Orientation Estimation        | Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models     | Zehan Wang                                | Introduces Orient Anything, a foundational model for estimating object orientation from a single image, using a large dataset of rendered 3D objects to train robust orientation estimation models, achieving state-of-the-art accuracy.                                            | [Link](http://arxiv.org/abs/2412.18605v1) |\\n| Explainable AI                       | Explaining in Diffusion: Explaining a Classifier Through Hierarchical Semantics with Text-to-Image Diffusion Models | Tahira Kazimi                             | Proposes DiffEx, which uses text-to-image diffusion models to explain classifier decisions, providing hierarchical and detailed explanations for complex scenes and multiple concepts, outperforming traditional GAN-based models.                                           | [Link](http://arxiv.org/abs/2412.18604v1) |\\n| Speech Generation                    | Long-Form Speech Generation with Spoken Language Models                                    | Se Jin Park                               | Presents SpeechSSM, the first model to generate long-form spoken audio in a single session without text intermediates, addressing challenges in long-form speech generation and evaluation, and introducing new metrics and benchmarks.                                       | [Link](http://arxiv.org/abs/2412.18603v1) |\\n| GameFi and Decentralized Intelligence| Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems | Fernando Jia                              | Integrates advanced AI agents into GameFi platforms using large language models, enhancing player engagement and economic interaction, and establishing decentralized, consensus-driven ecosystems for community collaboration and creator monetization.                    | [Link](http://arxiv.org/abs/2412.18601v1) |\\n| Human-Scene Interaction              | ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation                           | Hongjie Li                                | Introduces ZeroHSI, enabling zero-shot 4D human-scene interaction synthesis by integrating video generation and neural human rendering, leveraging motion priors from state-of-the-art video generation models without requiring ground-truth motion data.                  | [Link](http://arxiv.org/abs/2412.18600v1) |\\n| Blockchain and Ruin Theory           | Double Spending Analysis of Nakamoto Consensus for Time-Varying Mining Rates with Ruin Theory | Mustafa Doger                             | Introduces a ruin-theoretical model for double spending analysis in Nakamoto consensus under time-varying mining rates, considering network delays and dynamic miner participation, and presenting numerical examples to validate the proposed method.                           | [Link](http://arxiv.org/abs/2412.18599v1) |\\n```', 'name': 'Scientist', 'role': 'user'}], summary='Here is a markdown table categorizing the papers based on their domains:\\n\\n```markdown\\n| Domain                               | Title                                                                                      | Authors                                   | Summary                                                                                                                                                                                                                                                                        | Link                                     |\\n|--------------------------------------|--------------------------------------------------------------------------------------------|-------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------|\\n| Video-Language Models                | Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models          | Jinhui Yi                                 | Introduces a Spatio-Temporal Alignment Block (STAB) for video-language understanding, reducing computational overhead significantly without using heavyweight encoders, achieving competitive performance for open-ended video question answering.                          | [Link](http://arxiv.org/abs/2412.18609v1) |\\n| 3D Generation and Reconstruction     | PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models      | Minghao Chen                              | Proposes PartGen, a novel approach for generating 3D objects composed of meaningful parts from text, images, or unstructured 3D objects using multi-view diffusion models for part segmentation and reconstruction.                                                          | [Link](http://arxiv.org/abs/2412.18608v1) |\\n| Autonomous Driving                   | DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers | Yuntao Chen                               | Introduces DrivingGPT, a multimodal autoregressive transformer for unified driving model simulation and trajectory planning, demonstrating strong performance in action-conditioned video generation and end-to-end planning.                                                   | [Link](http://arxiv.org/abs/2412.18607v1) |\\n| Quantum Field Theories               | Lattice T-duality from non-invertible symmetries in quantum spin chains                     | Salvatore D. Pace                         | Explores T-duality of the compact boson CFT in quantum spin chains, uncovering exact lattice T-duality and non-invertible symmetries, and examining the gapless phase diagram with special WZW model points.                                                               | [Link](http://arxiv.org/abs/2412.18606v1) |\\n| Object Orientation Estimation        | Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models     | Zehan Wang                                | Introduces Orient Anything, a foundational model for estimating object orientation from a single image, using a large dataset of rendered 3D objects to train robust orientation estimation models, achieving state-of-the-art accuracy.                                            | [Link](http://arxiv.org/abs/2412.18605v1) |\\n| Explainable AI                       | Explaining in Diffusion: Explaining a Classifier Through Hierarchical Semantics with Text-to-Image Diffusion Models | Tahira Kazimi                             | Proposes DiffEx, which uses text-to-image diffusion models to explain classifier decisions, providing hierarchical and detailed explanations for complex scenes and multiple concepts, outperforming traditional GAN-based models.                                           | [Link](http://arxiv.org/abs/2412.18604v1) |\\n| Speech Generation                    | Long-Form Speech Generation with Spoken Language Models                                    | Se Jin Park                               | Presents SpeechSSM, the first model to generate long-form spoken audio in a single session without text intermediates, addressing challenges in long-form speech generation and evaluation, and introducing new metrics and benchmarks.                                       | [Link](http://arxiv.org/abs/2412.18603v1) |\\n| GameFi and Decentralized Intelligence| Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems | Fernando Jia                              | Integrates advanced AI agents into GameFi platforms using large language models, enhancing player engagement and economic interaction, and establishing decentralized, consensus-driven ecosystems for community collaboration and creator monetization.                    | [Link](http://arxiv.org/abs/2412.18601v1) |\\n| Human-Scene Interaction              | ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation                           | Hongjie Li                                | Introduces ZeroHSI, enabling zero-shot 4D human-scene interaction synthesis by integrating video generation and neural human rendering, leveraging motion priors from state-of-the-art video generation models without requiring ground-truth motion data.                  | [Link](http://arxiv.org/abs/2412.18600v1) |\\n| Blockchain and Ruin Theory           | Double Spending Analysis of Nakamoto Consensus for Time-Varying Mining Rates with Ruin Theory | Mustafa Doger                             | Introduces a ruin-theoretical model for double spending analysis in Nakamoto consensus under time-varying mining rates, considering network delays and dynamic miner participation, and presenting numerical examples to validate the proposed method.                           | [Link](http://arxiv.org/abs/2412.18599v1) |\\n```', cost={'usage_including_cached_inference': {'total_cost': 0}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=[])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    manager, \n",
    "    message=\"Topic: LLM applications papers from last week. Requirement: 5 - 10 papers from different domains.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save chat messages for resuming later on using the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"content\": \"Topic: LLM applications papers from last week. Requirement: 5 - 10 papers from different domains.\", \"role\": \"user\", \"name\": \"Admin\"}, {\"content\": \"Sure! I will write a Python script to retrieve papers from the arXiv API based on the topic \\\"Large Language Models (LLM) applications\\\" and filter them to get the latest papers from the past week. The script will display the title, authors, abstract, and link for each paper.\\n\\n```python\\nimport requests\\nimport datetime\\nfrom dateutil.parser import parse\\n\\ndef get_arxiv_papers(query, max_results=10):\\n    base_url = 'http://export.arxiv.org/api/query?'\\n    query = f'search_query=all:{query}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending'\\n    \\n    response = requests.get(base_url + query)\\n    \\n    if response.status_code != 200:\\n        print(\\\"Error fetching data from arXiv API\\\")\\n        return\\n    \\n    feed = response.text\\n    start_pos = 0\\n    papers = []\\n\\n    while True:\\n        start_index = feed.find('<entry>', start_pos)\\n        if start_index == -1:\\n            break\\n        end_index = feed.find('</entry>', start_index)\\n        entry = feed[start_index:end_index]\\n        \\n        title_start = entry.find('<title>') + len('<title>')\\n        title_end = entry.find('</title>', title_start)\\n        title = entry[title_start:title_end].strip()\\n        \\n        authors_start = entry.find('<author>')\\n        authors_end = entry.find('</author>', authors_start)\\n        authors = []\\n        while authors_start != -1 and authors_start < authors_end:\\n            author_start = entry.find('<name>', authors_start) + len('<name>')\\n            author_end = entry.find('</name>', author_start)\\n            authors.append(entry[author_start:author_end].strip())\\n            authors_start = entry.find('<author>', author_end)\\n        \\n        abstract_start = entry.find('<summary>') + len('<summary>')\\n        abstract_end = entry.find('</summary>', abstract_start)\\n        abstract = entry[abstract_start:abstract_end].strip()\\n\\n        link_start = entry.find('<id>') + len('<id>')\\n        link_end = entry.find('</id>', link_start)\\n        link = entry[link_start:link_end].strip()\\n\\n        updated_start = entry.find('<updated>') + len('<updated>')\\n        updated_end = entry.find('</updated>', updated_start)\\n        updated = entry[updated_start:updated_end].strip()\\n\\n        papers.append({\\n            'title': title,\\n            'authors': authors,\\n            'abstract': abstract,\\n            'link': link,\\n            'updated': parse(updated)\\n        })\\n\\n        start_pos = end_index\\n    \\n    return papers\\n\\ndef filter_recent_papers(papers, days=7):\\n    recent_papers = []\\n    current_time = datetime.datetime.now(datetime.timezone.utc)\\n    for paper in papers:\\n        if (current_time - paper['updated']).days <= days:\\n            recent_papers.append(paper)\\n    return recent_papers\\n\\nquery = \\\"Large Language Models applications\\\"\\npapers = get_arxiv_papers(query, max_results=20)\\nrecent_papers = filter_recent_papers(papers)\\n\\nfor idx, paper in enumerate(recent_papers[:10]):\\n    print(f\\\"Paper {idx + 1}:\\\")\\n    print(f\\\"Title: {paper['title']}\\\")\\n    print(f\\\"Authors: {', '.join(paper['authors'])}\\\")\\n    print(f\\\"Abstract: {paper['abstract']}\\\")\\n    print(f\\\"Link: {paper['link']}\\\")\\n    print()\\n\\nif not recent_papers:\\n    print(\\\"No recent papers found on this topic.\\\")\\n```\\n\\nThis script fetches up to 20 papers with the query \\\"Large Language Models applications\\\" from the arXiv API, filters them to get the ones from the past week, and prints their title, authors, abstract, and link.\", \"role\": \"user\", \"name\": \"Engineer\"}, {\"content\": \"exitcode: 0 (execution succeeded)\\nCode output: \\nPaper 1:\\nTitle: Video-Panda: Parameter-efficient Alignment for Encoder-free\\n  Video-Language Models\\nAuthors: Jinhui Yi\\nAbstract: We present an efficient encoder-free approach for video-language\\nunderstanding that achieves competitive performance while significantly\\nreducing computational overhead. Current video-language models typically rely\\non heavyweight image encoders (300M-1.1B parameters) or video encoders (1B-1.4B\\nparameters), creating a substantial computational burden when processing\\nmulti-frame videos. Our method introduces a novel Spatio-Temporal Alignment\\nBlock (STAB) that directly processes video inputs without requiring pre-trained\\nencoders while using only 45M parameters for visual processing - at least a\\n6.5$\\\\times$ reduction compared to traditional approaches. The STAB architecture\\ncombines Local Spatio-Temporal Encoding for fine-grained feature extraction,\\nefficient spatial downsampling through learned attention and separate\\nmechanisms for modeling frame-level and video-level relationships. Our model\\nachieves comparable or superior performance to encoder-based approaches for\\nopen-ended video question answering on standard benchmarks. The fine-grained\\nvideo question-answering evaluation demonstrates our model's effectiveness,\\noutperforming the encoder-based approaches Video-ChatGPT and Video-LLaVA in key\\naspects like correctness and temporal understanding. Extensive ablation studies\\nvalidate our architectural choices and demonstrate the effectiveness of our\\nspatio-temporal modeling approach while achieving 3-4$\\\\times$ faster processing\\nspeeds than previous methods. Code is available at\\n\\\\url{https://github.com/jh-yi/Video-Panda}.\\nLink: http://arxiv.org/abs/2412.18609v1\\n\\nPaper 2:\\nTitle: PartGen: Part-level 3D Generation and Reconstruction with Multi-View\\n  Diffusion Models\\nAuthors: Minghao Chen\\nAbstract: Text- or image-to-3D generators and 3D scanners can now produce 3D assets\\nwith high-quality shapes and textures. These assets typically consist of a\\nsingle, fused representation, like an implicit neural field, a Gaussian\\nmixture, or a mesh, without any useful structure. However, most applications\\nand creative workflows require assets to be made of several meaningful parts\\nthat can be manipulated independently. To address this gap, we introduce\\nPartGen, a novel approach that generates 3D objects composed of meaningful\\nparts starting from text, an image, or an unstructured 3D object. First, given\\nmultiple views of a 3D object, generated or rendered, a multi-view diffusion\\nmodel extracts a set of plausible and view-consistent part segmentations,\\ndividing the object into parts. Then, a second multi-view diffusion model takes\\neach part separately, fills in the occlusions, and uses those completed views\\nfor 3D reconstruction by feeding them to a 3D reconstruction network. This\\ncompletion process considers the context of the entire object to ensure that\\nthe parts integrate cohesively. The generative completion model can make up for\\nthe information missing due to occlusions; in extreme cases, it can hallucinate\\nentirely invisible parts based on the input 3D asset. We evaluate our method on\\ngenerated and real 3D assets and show that it outperforms segmentation and\\npart-extraction baselines by a large margin. We also showcase downstream\\napplications such as 3D part editing.\\nLink: http://arxiv.org/abs/2412.18608v1\\n\\nPaper 3:\\nTitle: DrivingGPT: Unifying Driving World Modeling and Planning with\\n  Multi-modal Autoregressive Transformers\\nAuthors: Yuntao Chen\\nAbstract: World model-based searching and planning are widely recognized as a promising\\npath toward human-level physical intelligence. However, current driving world\\nmodels primarily rely on video diffusion models, which specialize in visual\\ngeneration but lack the flexibility to incorporate other modalities like\\naction. In contrast, autoregressive transformers have demonstrated exceptional\\ncapability in modeling multimodal data. Our work aims to unify both driving\\nmodel simulation and trajectory planning into a single sequence modeling\\nproblem. We introduce a multimodal driving language based on interleaved image\\nand action tokens, and develop DrivingGPT to learn joint world modeling and\\nplanning through standard next-token prediction. Our DrivingGPT demonstrates\\nstrong performance in both action-conditioned video generation and end-to-end\\nplanning, outperforming strong baselines on large-scale nuPlan and NAVSIM\\nbenchmarks.\\nLink: http://arxiv.org/abs/2412.18607v1\\n\\nPaper 4:\\nTitle: Lattice T-duality from non-invertible symmetries in quantum spin chains\\nAuthors: Salvatore D. Pace\\nAbstract: Dualities of quantum field theories are challenging to realize in lattice\\nmodels of qubits. In this work, we explore one of the simplest dualities,\\nT-duality of the compact boson CFT, and its realization in quantum spin chains.\\nIn the special case of the XX model, we uncover an exact lattice T-duality,\\nwhich is associated with a non-invertible symmetry that exchanges two lattice\\nU(1) symmetries. The latter symmetries flow to the momentum and winding U(1)\\nsymmetries with a mixed anomaly in the CFT. However, the charge operators of\\nthe two U(1) symmetries do not commute on the lattice and instead generate the\\nOnsager algebra. We discuss how some of the anomalies in the CFT are\\nnonetheless still exactly realized on the lattice and how the lattice U(1)\\nsymmetries enforce gaplessness. We further explore lattice deformations\\npreserving both U(1) symmetries and find a rich gapless phase diagram with\\nspecial $\\\\mathrm{Spin}(2k)_1$ WZW model points and whose phase transitions all\\nhave dynamical exponent ${z&gt;1}$.\\nLink: http://arxiv.org/abs/2412.18606v1\\n\\nPaper 5:\\nTitle: Orient Anything: Learning Robust Object Orientation Estimation from\\n  Rendering 3D Models\\nAuthors: Zehan Wang\\nAbstract: Orientation is a key attribute of objects, crucial for understanding their\\nspatial pose and arrangement in images. However, practical solutions for\\naccurate orientation estimation from a single image remain underexplored. In\\nthis work, we introduce Orient Anything, the first expert and foundational\\nmodel designed to estimate object orientation in a single- and free-view image.\\nDue to the scarcity of labeled data, we propose extracting knowledge from the\\n3D world. By developing a pipeline to annotate the front face of 3D objects and\\nrender images from random views, we collect 2M images with precise orientation\\nannotations. To fully leverage the dataset, we design a robust training\\nobjective that models the 3D orientation as probability distributions of three\\nangles and predicts the object orientation by fitting these distributions.\\nBesides, we employ several strategies to improve synthetic-to-real transfer.\\nOur model achieves state-of-the-art orientation estimation accuracy in both\\nrendered and real images and exhibits impressive zero-shot ability in various\\nscenarios. More importantly, our model enhances many applications, such as\\ncomprehension and generation of complex spatial concepts and 3D object pose\\nadjustment.\\nLink: http://arxiv.org/abs/2412.18605v1\\n\\nPaper 6:\\nTitle: Explaining in Diffusion: Explaining a Classifier Through Hierarchical\\n  Semantics with Text-to-Image Diffusion Models\\nAuthors: Tahira Kazimi\\nAbstract: Classifiers are important components in many computer vision tasks, serving\\nas the foundational backbone of a wide variety of models employed across\\ndiverse applications. However, understanding the decision-making process of\\nclassifiers remains a significant challenge. We propose DiffEx, a novel method\\nthat leverages the capabilities of text-to-image diffusion models to explain\\nclassifier decisions. Unlike traditional GAN-based explainability models, which\\nare limited to simple, single-concept analyses and typically require training a\\nnew model for each classifier, our approach can explain classifiers that focus\\non single concepts (such as faces or animals) as well as those that handle\\ncomplex scenes involving multiple concepts. DiffEx employs vision-language\\nmodels to create a hierarchical list of semantics, allowing users to identify\\nnot only the overarching semantic influences on classifiers (e.g., the 'beard'\\nsemantic in a facial classifier) but also their sub-types, such as 'goatee' or\\n'Balbo' beard. Our experiments demonstrate that DiffEx is able to cover a\\nsignificantly broader spectrum of semantics compared to its GAN counterparts,\\nproviding a hierarchical tool that delivers a more detailed and fine-grained\\nunderstanding of classifier decisions.\\nLink: http://arxiv.org/abs/2412.18604v1\\n\\nPaper 7:\\nTitle: Long-Form Speech Generation with Spoken Language Models\\nAuthors: Se Jin Park\\nAbstract: We consider the generative modeling of speech over multiple minutes, a\\nrequirement for long-form multimedia generation and audio-native voice\\nassistants. However, current spoken language models struggle to generate\\nplausible speech past tens of seconds, from high temporal resolution of speech\\ntokens causing loss of coherence, to architectural issues with long-sequence\\ntraining or extrapolation, to memory costs at inference time. With these\\nconsiderations we propose SpeechSSM, the first speech language model to learn\\nfrom and sample long-form spoken audio (e.g., 16 minutes of read or\\nextemporaneous speech) in a single decoding session without text intermediates,\\nbased on recent advances in linear-time sequence modeling. Furthermore, to\\naddress growing challenges in spoken language evaluation, especially in this\\nnew long-form setting, we propose: new embedding-based and LLM-judged metrics;\\nquality measurements over length and time; and a new benchmark for long-form\\nspeech processing and generation, LibriSpeech-Long. Speech samples and the\\ndataset are released at\\nhttps://google.github.io/tacotron/publications/speechssm/\\nLink: http://arxiv.org/abs/2412.18603v1\\n\\nPaper 8:\\nTitle: Decentralized Intelligence in GameFi: Embodied AI Agents and the\\n  Convergence of DeFi and Virtual Ecosystems\\nAuthors: Fernando Jia\\nAbstract: In the rapidly evolving landscape of GameFi, a fusion of gaming and\\ndecentralized finance (DeFi), there exists a critical need to enhance player\\nengagement and economic interaction within gaming ecosystems. Our GameFi\\necosystem aims to fundamentally transform this landscape by integrating\\nadvanced embodied AI agents into GameFi platforms. These AI agents, developed\\nusing cutting-edge large language models (LLMs), such as GPT-4 and Claude AI,\\nare capable of proactive, adaptive, and contextually rich interactions with\\nplayers. By going beyond traditional scripted responses, these agents become\\nintegral participants in the game's narrative and economic systems, directly\\ninfluencing player strategies and in-game economies. We address the limitations\\nof current GameFi platforms, which often lack immersive AI interactions and\\nmechanisms for community engagement or creator monetization. Through the deep\\nintegration of AI agents with blockchain technology, we establish a\\nconsensus-driven, decentralized GameFi ecosystem. This ecosystem empowers\\ncreators to monetize their contributions and fosters democratic collaboration\\namong players and creators. Furthermore, by embedding DeFi mechanisms into the\\ngaming experience, we enhance economic participation and provide new\\nopportunities for financial interactions within the game. Our approach enhances\\nplayer immersion and retention and advances the GameFi ecosystem by bridging\\ntraditional gaming with Web3 technologies. By integrating sophisticated AI and\\nDeFi elements, we contribute to the development of more engaging, economically\\nrobust, and community-centric gaming environments. This project represents a\\nsignificant advancement in the state-of-the-art in GameFi, offering insights\\nand methodologies that can be applied throughout the gaming industry.\\nLink: http://arxiv.org/abs/2412.18601v1\\n\\nPaper 9:\\nTitle: ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation\\nAuthors: Hongjie Li\\nAbstract: Human-scene interaction (HSI) generation is crucial for applications in\\nembodied AI, virtual reality, and robotics. While existing methods can\\nsynthesize realistic human motions in 3D scenes and generate plausible\\nhuman-object interactions, they heavily rely on datasets containing paired 3D\\nscene and motion capture data, which are expensive and time-consuming to\\ncollect across diverse environments and interactions. We present ZeroHSI, a\\nnovel approach that enables zero-shot 4D human-scene interaction synthesis by\\nintegrating video generation and neural human rendering. Our key insight is to\\nleverage the rich motion priors learned by state-of-the-art video generation\\nmodels, which have been trained on vast amounts of natural human movements and\\ninteractions, and use differentiable rendering to reconstruct human-scene\\ninteractions. ZeroHSI can synthesize realistic human motions in both static\\nscenes and environments with dynamic objects, without requiring any\\nground-truth motion data. We evaluate ZeroHSI on a curated dataset of different\\ntypes of various indoor and outdoor scenes with different interaction prompts,\\ndemonstrating its ability to generate diverse and contextually appropriate\\nhuman-scene interactions.\\nLink: http://arxiv.org/abs/2412.18600v1\\n\\nPaper 10:\\nTitle: Double Spending Analysis of Nakamoto Consensus for Time-Varying Mining\\n  Rates with Ruin Theory\\nAuthors: Mustafa Doger\\nAbstract: Theoretical guarantees for double spending probabilities for the Nakamoto\\nconsensus under the $k$-deep confirmation rule have been extensively studied\\nfor zero/bounded network delays and fixed mining rates. In this paper, we\\nintroduce a ruin-theoretical model of double spending for Nakamoto consensus\\nunder the $k$-deep confirmation rule when the honest mining rate is allowed to\\nbe an arbitrary function of time including the block delivery periods, i.e.,\\ntime periods during which mined blocks are being delivered to all other\\nparticipants of the network. Time-varying mining rates are considered to\\ncapture the intrinsic characteristics of the peer to peer network delays as\\nwell as dynamic participation of miners such as the gap game and switching\\nbetween different cryptocurrencies. Ruin theory is leveraged to obtain the\\ndouble spend probabilities and numerical examples are presented to validate the\\neffectiveness of the proposed analytical method.\\nLink: http://arxiv.org/abs/2412.18599v1\\n\\n\", \"role\": \"user\", \"name\": \"Executor\"}, {\"content\": \"Here is a markdown table categorizing the papers based on their domains:\\n\\n```markdown\\n| Domain                               | Title                                                                                      | Authors                                   | Summary                                                                                                                                                                                                                                                                        | Link                                     |\\n|--------------------------------------|--------------------------------------------------------------------------------------------|-------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------|\\n| Video-Language Models                | Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models          | Jinhui Yi                                 | Introduces a Spatio-Temporal Alignment Block (STAB) for video-language understanding, reducing computational overhead significantly without using heavyweight encoders, achieving competitive performance for open-ended video question answering.                          | [Link](http://arxiv.org/abs/2412.18609v1) |\\n| 3D Generation and Reconstruction     | PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models      | Minghao Chen                              | Proposes PartGen, a novel approach for generating 3D objects composed of meaningful parts from text, images, or unstructured 3D objects using multi-view diffusion models for part segmentation and reconstruction.                                                          | [Link](http://arxiv.org/abs/2412.18608v1) |\\n| Autonomous Driving                   | DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers | Yuntao Chen                               | Introduces DrivingGPT, a multimodal autoregressive transformer for unified driving model simulation and trajectory planning, demonstrating strong performance in action-conditioned video generation and end-to-end planning.                                                   | [Link](http://arxiv.org/abs/2412.18607v1) |\\n| Quantum Field Theories               | Lattice T-duality from non-invertible symmetries in quantum spin chains                     | Salvatore D. Pace                         | Explores T-duality of the compact boson CFT in quantum spin chains, uncovering exact lattice T-duality and non-invertible symmetries, and examining the gapless phase diagram with special WZW model points.                                                               | [Link](http://arxiv.org/abs/2412.18606v1) |\\n| Object Orientation Estimation        | Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models     | Zehan Wang                                | Introduces Orient Anything, a foundational model for estimating object orientation from a single image, using a large dataset of rendered 3D objects to train robust orientation estimation models, achieving state-of-the-art accuracy.                                            | [Link](http://arxiv.org/abs/2412.18605v1) |\\n| Explainable AI                       | Explaining in Diffusion: Explaining a Classifier Through Hierarchical Semantics with Text-to-Image Diffusion Models | Tahira Kazimi                             | Proposes DiffEx, which uses text-to-image diffusion models to explain classifier decisions, providing hierarchical and detailed explanations for complex scenes and multiple concepts, outperforming traditional GAN-based models.                                           | [Link](http://arxiv.org/abs/2412.18604v1) |\\n| Speech Generation                    | Long-Form Speech Generation with Spoken Language Models                                    | Se Jin Park                               | Presents SpeechSSM, the first model to generate long-form spoken audio in a single session without text intermediates, addressing challenges in long-form speech generation and evaluation, and introducing new metrics and benchmarks.                                       | [Link](http://arxiv.org/abs/2412.18603v1) |\\n| GameFi and Decentralized Intelligence| Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems | Fernando Jia                              | Integrates advanced AI agents into GameFi platforms using large language models, enhancing player engagement and economic interaction, and establishing decentralized, consensus-driven ecosystems for community collaboration and creator monetization.                    | [Link](http://arxiv.org/abs/2412.18601v1) |\\n| Human-Scene Interaction              | ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation                           | Hongjie Li                                | Introduces ZeroHSI, enabling zero-shot 4D human-scene interaction synthesis by integrating video generation and neural human rendering, leveraging motion priors from state-of-the-art video generation models without requiring ground-truth motion data.                  | [Link](http://arxiv.org/abs/2412.18600v1) |\\n| Blockchain and Ruin Theory           | Double Spending Analysis of Nakamoto Consensus for Time-Varying Mining Rates with Ruin Theory | Mustafa Doger                             | Introduces a ruin-theoretical model for double spending analysis in Nakamoto consensus under time-varying mining rates, considering network delays and dynamic miner participation, and presenting numerical examples to validate the proposed method.                           | [Link](http://arxiv.org/abs/2412.18599v1) |\\n```\", \"role\": \"user\", \"name\": \"Scientist\"}]\n"
     ]
    }
   ],
   "source": [
    "previous_state = manager.messages_to_string(manager.groupchat.messages)\n",
    "print(previous_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resume, the agents, GroupChat, and GroupChatManager objects must exist and match the original group chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When preparing for resuming, the messages will be validated against the groupchat’s agents to make sure that the messages can be assigned to them. Messages will be allocated to the agents and then the last speaker and message will be returned for use in `initiate_chat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continuing a terminated conversation**\n",
    "\n",
    "- If the previous group chat terminated and the resuming group chat has the same termination condition (such as if the message contains “TERMINATE”) then the conversation will terminate when resuming as the terminate check occurs with the message passed in to `initiate_chat`.\n",
    "\n",
    "- If the termination condition is based on a string within the message, you can pass in that string in the `remove_termination_string` parameter of the `resume` function and it will be removed. If the termination condition is more complicated, you will need to adjust the messages accordingly before calling `resume`.\n",
    "\n",
    "- The `resume` function will then check if the last message provided still meets the termination condition and warns you, if so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the code to resume a conversation from previous state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = autogen.AssistantAgent(\n",
    "    name=\"Planner\",\n",
    "    system_message=\"\"\"Planner. Suggest a plan. Revise the plan based on feedback from admin and critic, until admin approval.\n",
    "The plan may involve an engineer who can write code and a scientist who doesn't write code.\n",
    "Explain the plan first. Be clear which step is performed by an engineer, and which step is performed by a scientist.\n",
    "\"\"\",\n",
    "    llm_config={\"config_list\": config_list}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"Admin\",\n",
    "    system_message=\"A human admin. Interact with the planner to discuss the plan. Plan execution needs to be approved by this admin.\",\n",
    "    code_execution_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    llm_config= {\"config_list\": config_list},\n",
    "    system_message=\"\"\"Engineer. You follow an approved plan. You write python/shell code to solve tasks. Wrap the code in a code block that specifies the script type. The user can't modify your code. So do not suggest incomplete code which requires others to modify. Don't use a code block if it's not intended to be executed by the executor.\n",
    "Don't include multiple code blocks in one response. Do not ask others to copy and paste the result. Check the execution result returned by the executor.\n",
    "If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "scientist = autogen.AssistantAgent(\n",
    "    name=\"Scientist\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    system_message=\"\"\"Scientist. You follow an approved plan. You are able to categorize papers after seeing their abstracts printed. You don't write code.\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=\"Executor. Execute the code written by the engineer and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, engineer, scientist, planner, executor],\n",
    "    messages=[],\n",
    "    max_round=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config= { \"config_list\": config_list })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared group chat with 4 messages, the last speaker is \u001b[33mScientist\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Prepare the group chat for resuming\n",
    "last_agent, last_message = manager.resume(messages=previous_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mScientist\u001b[0m (to chat_manager):\n",
      "\n",
      "Here is a markdown table categorizing the papers based on their domains:\n",
      "\n",
      "```markdown\n",
      "| Domain                               | Title                                                                                      | Authors                                   | Summary                                                                                                                                                                                                                                                                        | Link                                     |\n",
      "|--------------------------------------|--------------------------------------------------------------------------------------------|-------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------|\n",
      "| Video-Language Models                | Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models          | Jinhui Yi                                 | Introduces a Spatio-Temporal Alignment Block (STAB) for video-language understanding, reducing computational overhead significantly without using heavyweight encoders, achieving competitive performance for open-ended video question answering.                          | [Link](http://arxiv.org/abs/2412.18609v1) |\n",
      "| 3D Generation and Reconstruction     | PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models      | Minghao Chen                              | Proposes PartGen, a novel approach for generating 3D objects composed of meaningful parts from text, images, or unstructured 3D objects using multi-view diffusion models for part segmentation and reconstruction.                                                          | [Link](http://arxiv.org/abs/2412.18608v1) |\n",
      "| Autonomous Driving                   | DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers | Yuntao Chen                               | Introduces DrivingGPT, a multimodal autoregressive transformer for unified driving model simulation and trajectory planning, demonstrating strong performance in action-conditioned video generation and end-to-end planning.                                                   | [Link](http://arxiv.org/abs/2412.18607v1) |\n",
      "| Quantum Field Theories               | Lattice T-duality from non-invertible symmetries in quantum spin chains                     | Salvatore D. Pace                         | Explores T-duality of the compact boson CFT in quantum spin chains, uncovering exact lattice T-duality and non-invertible symmetries, and examining the gapless phase diagram with special WZW model points.                                                               | [Link](http://arxiv.org/abs/2412.18606v1) |\n",
      "| Object Orientation Estimation        | Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models     | Zehan Wang                                | Introduces Orient Anything, a foundational model for estimating object orientation from a single image, using a large dataset of rendered 3D objects to train robust orientation estimation models, achieving state-of-the-art accuracy.                                            | [Link](http://arxiv.org/abs/2412.18605v1) |\n",
      "| Explainable AI                       | Explaining in Diffusion: Explaining a Classifier Through Hierarchical Semantics with Text-to-Image Diffusion Models | Tahira Kazimi                             | Proposes DiffEx, which uses text-to-image diffusion models to explain classifier decisions, providing hierarchical and detailed explanations for complex scenes and multiple concepts, outperforming traditional GAN-based models.                                           | [Link](http://arxiv.org/abs/2412.18604v1) |\n",
      "| Speech Generation                    | Long-Form Speech Generation with Spoken Language Models                                    | Se Jin Park                               | Presents SpeechSSM, the first model to generate long-form spoken audio in a single session without text intermediates, addressing challenges in long-form speech generation and evaluation, and introducing new metrics and benchmarks.                                       | [Link](http://arxiv.org/abs/2412.18603v1) |\n",
      "| GameFi and Decentralized Intelligence| Decentralized Intelligence in GameFi: Embodied AI Agents and the Convergence of DeFi and Virtual Ecosystems | Fernando Jia                              | Integrates advanced AI agents into GameFi platforms using large language models, enhancing player engagement and economic interaction, and establishing decentralized, consensus-driven ecosystems for community collaboration and creator monetization.                    | [Link](http://arxiv.org/abs/2412.18601v1) |\n",
      "| Human-Scene Interaction              | ZeroHSI: Zero-Shot 4D Human-Scene Interaction by Video Generation                           | Hongjie Li                                | Introduces ZeroHSI, enabling zero-shot 4D human-scene interaction synthesis by integrating video generation and neural human rendering, leveraging motion priors from state-of-the-art video generation models without requiring ground-truth motion data.                  | [Link](http://arxiv.org/abs/2412.18600v1) |\n",
      "| Blockchain and Ruin Theory           | Double Spending Analysis of Nakamoto Consensus for Time-Varying Mining Rates with Ruin Theory | Mustafa Doger                             | Introduces a ruin-theoretical model for double spending analysis in Nakamoto consensus under time-varying mining rates, considering network delays and dynamic miner participation, and presenting numerical examples to validate the proposed method.                           | [Link](http://arxiv.org/abs/2412.18599v1) |\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "Approve\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Planner\n",
      "\u001b[0m\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "The plan is approved. The next steps are to share the summarized papers with the relevant stakeholders and utilize or further analyze these findings for ongoing research or development.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "Approve\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "Approve\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "Approve\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Resume the chat using the last agent and message\n",
    "result = last_agent.initiate_chat(recipient=manager, message=last_message, clear_history=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE - Sometimes, the agents are not selected correctly. For best results, use your own `state_transition` speaker selection method as used previously.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1, Admin: Topic: LLM applications papers from last week. Requirement: 5 - 10 papers from d ...\n",
      "#2, Engineer: Sure! I will write a Python script to retrieve papers from the arXiv API based o ...\n",
      "#3, Executor: exitcode: 0 (execution succeeded) Code output:  Paper 1: Title: Video-Panda: Par ...\n",
      "#4, Scientist: Here is a markdown table categorizing the papers based on their domains:  ```mar ...\n",
      "#5, Admin: Approve \n",
      "#6, Planner: The plan is approved. The next steps are to share the summarized papers with the ...\n",
      "#7, Admin: Approve \n",
      "#8, Admin: Approve \n",
      "#9, Admin: Approve \n",
      "#10, Admin:  \n",
      "#11, Admin:  \n",
      "#12, Admin:  \n",
      "#13, Admin:  \n"
     ]
    }
   ],
   "source": [
    "# Output the final chat history showing the original 4 messages and resumed messages\n",
    "for i, message in enumerate(groupchat.messages):\n",
    "    print(\n",
    "        f\"#{i + 1}, {message['name']}: {message['content'][:80]}\".replace(\"\\n\", \" \"),\n",
    "        f\"{'...' if len(message['content']) > 80 else ''}\".replace(\"\\n\", \" \"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
